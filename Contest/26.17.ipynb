{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 28\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# train/val\n",
    "train_data = pd.read_csv('ttids21/train.csv')\n",
    "zipcode_data = pd.read_csv('ttids21/zipcodes.csv')\n",
    "zipcode_data.drop_duplicates(subset=['zipcode'], keep='first', inplace=True)\n",
    "\n",
    "X = train_data.drop(columns=['Unnamed: 0', 'price'])\n",
    "y = train_data.price\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.7, random_state=SEED)\n",
    "\n",
    "def f(x):\n",
    "    if x < 20:\n",
    "        return 2000 + x\n",
    "    if x < 100:\n",
    "        return 1900 + x\n",
    "    return x\n",
    "\n",
    "train_X.registration_year = train_X.registration_year.map(f)\n",
    "val_X.registration_year = val_X.registration_year.map(f)\n",
    "\n",
    "train_X = pd.merge(train_X, zipcode_data, left_on='zipcode', right_on='zipcode', how=\"left\")\n",
    "val_X = pd.merge(val_X, zipcode_data, left_on='zipcode', right_on='zipcode', how=\"left\")\n",
    "\n",
    "my_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "imputed_train_X = pd.DataFrame(my_imputer.fit_transform(train_X))\n",
    "imputed_val_X = pd.DataFrame(my_imputer.transform(val_X))\n",
    "\n",
    "imputed_train_X.columns = train_X.columns\n",
    "imputed_val_X.columns = val_X.columns\n",
    "\n",
    "clustering_data = imputed_train_X[['latitude', 'longitude']]\n",
    "cluster_picker = KMeans(n_clusters=200)\n",
    "clusters = cluster_picker.fit_predict(clustering_data)\n",
    "imputed_train_X['geo_cluster'] = clusters\n",
    "clustering_data = imputed_val_X[['latitude', 'longitude']]\n",
    "imputed_val_X['geo_cluster'] = cluster_picker.predict(clustering_data)\n",
    "\n",
    "imputed_train_X.drop(columns=['Unnamed: 0', 'zipcode', 'city', 'latitude', 'longitude'])\n",
    "imputed_val_X.drop(columns=['Unnamed: 0', 'zipcode', 'city', 'latitude', 'longitude'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "columns_to_transform = ['type', 'model', 'fuel', 'brand']\n",
    "\n",
    "d = dict()\n",
    "label_encoders = dict()\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(np.append(imputed_train_X[col], ['-1']))\n",
    "    label_encoders[col] = le\n",
    "    imputed_train_X[col] = le.transform(imputed_train_X[col])\n",
    "    classes = le.classes_\n",
    "    d[col] = len(le.classes_)\n",
    "    unknown_classes = set(imputed_val_X[col]) - set(classes)\n",
    "    imputed_val_X[col].replace(unknown_classes, '-1', inplace=True)\n",
    "    imputed_val_X[col] = le.transform(imputed_val_X[col])\n",
    "\n",
    "imputed_train_X.replace(['manual', 'auto'], [0, 1], inplace=True)\n",
    "imputed_val_X.replace(['manual', 'auto'], [0, 1], inplace=True)\n",
    "\n",
    "columns_to_remove = ['zipcode', 'Unnamed: 0', 'city', 'latitude', 'longitude']\n",
    "\n",
    "imputed_train_X.drop(columns=columns_to_remove, inplace=True)\n",
    "imputed_val_X.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# test\n",
    "test_data = pd.read_csv('ttids21/test_no_target.csv')\n",
    "\n",
    "test_data.index = test_data['Unnamed: 0']\n",
    "indexes = test_data.index\n",
    "\n",
    "test_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "test_data.registration_year = test_data.registration_year.map(f)\n",
    "\n",
    "test_data = pd.merge(test_data, zipcode_data, left_on='zipcode', right_on='zipcode', how=\"left\")\n",
    "test_data.index = indexes\n",
    "\n",
    "imputed_test = pd.DataFrame(my_imputer.transform(test_data))\n",
    "imputed_test.columns = test_data.columns\n",
    "imputed_test.index = indexes\n",
    "\n",
    "clustering_data = imputed_test[['latitude', 'longitude']]\n",
    "imputed_test['geo_cluster'] = cluster_picker.predict(clustering_data)\n",
    "imputed_test.drop(columns=['Unnamed: 0', 'zipcode', 'city', 'latitude', 'longitude'])\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    le = label_encoders[col]\n",
    "    classes = le.classes_\n",
    "    unknown_classes = set(imputed_test[col]) - set(classes)\n",
    "    imputed_test[col].replace(unknown_classes, '-1', inplace=True)\n",
    "    imputed_test[col] = le.transform(imputed_test[col])\n",
    "\n",
    "imputed_test.replace(['manual', 'auto'], [0, 1], inplace=True)\n",
    "imputed_test.drop(columns=columns_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17F2~1\\AppData\\Local\\Temp/ipykernel_1996/1742191283.py:32: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  torch.nn.init.kaiming_uniform(m.weight)\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_samples = x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x.iloc[index].values, self.y.iloc[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        self.n_samples = x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x.iloc[index].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# n_types = imputed_train_X.type.unique()\n",
    "# n_models = imputed_train_X.model.unique()\n",
    "# n_fuels = imputed_train_X.fuel.unique()\n",
    "# n_brands = imputed_train_X.type.unique()\n",
    "n_geo_points = cluster_picker.n_clusters\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "size1 = 128\n",
    "size2 = 1024\n",
    "size3 = 6*size1 + size2\n",
    "size4 = 2048\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.binary_branch = nn.Sequential(\n",
    "            nn.Linear(2, size1),\n",
    "            nn.BatchNorm1d(num_features=size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            nn.BatchNorm1d(num_features=size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.features_branch = nn.Sequential(\n",
    "            nn.Linear(5, size2),\n",
    "            nn.BatchNorm1d(num_features=size2),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size2, size2),\n",
    "            nn.BatchNorm1d(num_features=size2),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size2, size2),\n",
    "            nn.BatchNorm1d(num_features=size2),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.type_branch = nn.Sequential(\n",
    "            nn.Embedding(d['type'], size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.model_branch = nn.Sequential(\n",
    "            nn.Embedding(d['model'], size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fuel_branch = nn.Sequential(\n",
    "            nn.Embedding(d['fuel'], size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.brand_branch = nn.Sequential(\n",
    "            nn.Embedding(d['brand'], size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.geo_branch = nn.Sequential(\n",
    "            nn.Embedding(n_geo_points, size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.main_branch = nn.Sequential(\n",
    "            nn.Linear(size3, size4),\n",
    "            nn.BatchNorm1d(num_features=size4),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size4, size4),\n",
    "            nn.BatchNorm1d(num_features=size4),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size4, size4),\n",
    "            nn.BatchNorm1d(num_features=size4),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size4, 1)\n",
    "        )\n",
    "        \n",
    "        self.binary_branch.apply(init_weights)\n",
    "        self.features_branch.apply(init_weights)\n",
    "        self.type_branch.apply(init_weights)\n",
    "        self.model_branch.apply(init_weights)\n",
    "        self.fuel_branch.apply(init_weights)\n",
    "        self.brand_branch.apply(init_weights)\n",
    "        self.geo_branch.apply(init_weights)\n",
    "        self.main_branch.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.binary_branch(x[:, [3, 9]].to(torch.float32))\n",
    "        out2 = self.features_branch(x[:, [0, 2, 4, 6, 10]].to(torch.float32))\n",
    "        out3 = self.type_branch(x[:, 1].to(torch.long))\n",
    "        out4 = self.model_branch(x[:, 5].to(torch.long))\n",
    "        out5 = self.fuel_branch(x[:, 7].to(torch.long))\n",
    "        out6 = self.brand_branch(x[:, 8].to(torch.long))\n",
    "        out7 = self.geo_branch(x[:, 11].to(torch.long))\n",
    "        out = torch.cat((out1, out2, out3, out4, out5, out6, out7), 1)\n",
    "        out = self.main_branch(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def mean_absolute_percentage_error(y_pred, y_true):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(imputed_train_X, train_y)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = CustomDataset(imputed_val_X, val_y)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "n_epochs = 100\n",
    "train_epoch_loss = np.zeros(n_epochs, dtype=np.float32)\n",
    "val_epoch_loss = np.zeros(n_epochs, dtype=np.float32)\n",
    "criterion = mean_absolute_percentage_error\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, nesterov=True, weight_decay= 0.0001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.000001)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.05, mode='exp_range', gamma=0.99994, step_size_up=len(train_dataloader)*2)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr_max, steps_per_epoch=len(train_dataloader), epochs=n_epochs)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "# actual_lr = lr_max\n",
    "# low_lr = True\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    n_iterations = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    y_pred = torch.tensor([]).to(device)\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y = y.reshape(y.shape[0], 1).type(torch.float32)\n",
    "\n",
    "        pred = model(x)\n",
    "        pred = pred.to(device)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        y_pred = torch.cat((y_pred, pred))\n",
    "        y_true = torch.cat((y_true, y))\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Step {i+1} / {n_iterations}, loss = {loss.item():.4f}')\n",
    "    \n",
    "    train_loss = loss_fn(y_pred, y_true)\n",
    "    print(f'Train loss: {train_loss:>8f}')\n",
    "    return train_loss\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y_pred = torch.tensor([]).to(device)\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y = y.reshape(y.shape[0], 1).to(torch.float32)\n",
    "            \n",
    "            pred = model(x)\n",
    "            pred = pred.to(device)\n",
    "            y_pred = torch.cat((y_pred, pred))\n",
    "            y_true = torch.cat((y_true, y))\n",
    "\n",
    "    # y_pred = torch.tensor(y_pred).to(device)\n",
    "    # y_true = torch.tensor(y_true).to(device)\n",
    "    test_loss = loss_fn(y_pred, y_true)\n",
    "    print(f'Validation loss: {test_loss:>8f} \\n')\n",
    "    return test_loss\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    print(\"Saving checkpoint...\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint):\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-6]\n",
    "while learning_rates[-1] < 1:\n",
    "    learning_rates.append(learning_rates[-1] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-06,\n",
       " 2e-06,\n",
       " 4e-06,\n",
       " 8e-06,\n",
       " 1.6e-05,\n",
       " 3.2e-05,\n",
       " 6.4e-05,\n",
       " 0.000128,\n",
       " 0.000256,\n",
       " 0.000512,\n",
       " 0.001024,\n",
       " 0.002048,\n",
       " 0.004096,\n",
       " 0.008192,\n",
       " 0.016384,\n",
       " 0.032768,\n",
       " 0.065536,\n",
       " 0.131072,\n",
       " 0.262144,\n",
       " 0.524288,\n",
       " 1.048576]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\17F2~1\\AppData\\Local\\Temp/ipykernel_10840/1033684717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlr_max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_max' is not defined"
     ]
    }
   ],
   "source": [
    "lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 90.1300\n",
      "Step 200 / 547, loss = 75.7449\n",
      "Step 300 / 547, loss = 72.8898\n",
      "Step 400 / 547, loss = 69.2665\n",
      "Step 500 / 547, loss = 61.2713\n",
      "Train loss: 75.891205\n",
      "Validation loss: 66.432190 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 54.4435\n",
      "Step 200 / 547, loss = 52.7624\n",
      "Step 300 / 547, loss = 48.8156\n",
      "Step 400 / 547, loss = 52.4656\n",
      "Step 500 / 547, loss = 55.3066\n",
      "Train loss: 54.321598\n",
      "Validation loss: 65.044556 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 41.0321\n",
      "Step 200 / 547, loss = 42.6372\n",
      "Step 300 / 547, loss = 39.2091\n",
      "Step 400 / 547, loss = 41.4402\n",
      "Step 500 / 547, loss = 36.8801\n",
      "Train loss: 43.976395\n",
      "Validation loss: 41.077545 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 32.3226\n",
      "Step 200 / 547, loss = 34.0201\n",
      "Step 300 / 547, loss = 38.9782\n",
      "Step 400 / 547, loss = 40.3077\n",
      "Step 500 / 547, loss = 36.5625\n",
      "Train loss: 36.869370\n",
      "Validation loss: 36.334496 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 30.9851\n",
      "Step 200 / 547, loss = 37.7932\n",
      "Step 300 / 547, loss = 32.0668\n",
      "Step 400 / 547, loss = 36.3907\n",
      "Step 500 / 547, loss = 30.1687\n",
      "Train loss: 33.614040\n",
      "Validation loss: 127.367004 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 30.0899\n",
      "Step 200 / 547, loss = 30.1930\n",
      "Step 300 / 547, loss = 30.4693\n",
      "Step 400 / 547, loss = 32.3574\n",
      "Step 500 / 547, loss = 26.7498\n",
      "Train loss: 32.288704\n",
      "Validation loss: 55.171406 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 39.9570\n",
      "Step 200 / 547, loss = 31.2769\n",
      "Step 300 / 547, loss = 32.4264\n",
      "Step 400 / 547, loss = 29.4336\n",
      "Step 500 / 547, loss = 30.9636\n",
      "Train loss: 31.414324\n",
      "Validation loss: 31.803020 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 26.4830\n",
      "Step 200 / 547, loss = 31.9132\n",
      "Step 300 / 547, loss = 37.7885\n",
      "Step 400 / 547, loss = 29.1473\n",
      "Step 500 / 547, loss = 31.5763\n",
      "Train loss: 30.882559\n",
      "Validation loss: 37.169548 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 34.3150\n",
      "Step 200 / 547, loss = 27.6279\n",
      "Step 300 / 547, loss = 26.6781\n",
      "Step 400 / 547, loss = 33.8490\n",
      "Step 500 / 547, loss = 28.8314\n",
      "Train loss: 30.576614\n",
      "Validation loss: 32.381596 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 32.0973\n",
      "Step 200 / 547, loss = 27.0221\n",
      "Step 300 / 547, loss = 27.7506\n",
      "Step 400 / 547, loss = 26.9632\n",
      "Step 500 / 547, loss = 30.6516\n",
      "Train loss: 30.224859\n",
      "Validation loss: 69.749313 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 30.0517\n",
      "Step 200 / 547, loss = 28.1361\n",
      "Step 300 / 547, loss = 31.4906\n",
      "Step 400 / 547, loss = 28.7506\n",
      "Step 500 / 547, loss = 29.1124\n",
      "Train loss: 29.834726\n",
      "Validation loss: 31.924856 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 29.5396\n",
      "Step 200 / 547, loss = 29.7622\n",
      "Step 300 / 547, loss = 35.0198\n",
      "Step 400 / 547, loss = 33.1467\n",
      "Step 500 / 547, loss = 29.2936\n",
      "Train loss: 29.873625\n",
      "Validation loss: 51.968121 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 29.3173\n",
      "Step 200 / 547, loss = 24.6316\n",
      "Step 300 / 547, loss = 30.6517\n",
      "Step 400 / 547, loss = 27.2465\n",
      "Step 500 / 547, loss = 29.9919\n",
      "Train loss: 29.529255\n",
      "Validation loss: 30.013731 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 32.3010\n",
      "Step 200 / 547, loss = 29.5199\n",
      "Step 300 / 547, loss = 41.0961\n",
      "Step 400 / 547, loss = 29.3389\n",
      "Step 500 / 547, loss = 31.3859\n",
      "Train loss: 29.109446\n",
      "Validation loss: 31.683270 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 36.6517\n",
      "Step 200 / 547, loss = 28.4611\n",
      "Step 300 / 547, loss = 28.9893\n",
      "Step 400 / 547, loss = 39.9562\n",
      "Step 500 / 547, loss = 29.3883\n",
      "Train loss: 28.671276\n",
      "Validation loss: 33.577927 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 31.9430\n",
      "Step 200 / 547, loss = 22.6003\n",
      "Step 300 / 547, loss = 25.7248\n",
      "Step 400 / 547, loss = 28.4315\n",
      "Step 500 / 547, loss = 23.4836\n",
      "Train loss: 28.532223\n",
      "Validation loss: 41.743248 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 36.2530\n",
      "Step 200 / 547, loss = 29.0773\n",
      "Step 300 / 547, loss = 28.4813\n",
      "Step 400 / 547, loss = 28.7452\n",
      "Step 500 / 547, loss = 29.0346\n",
      "Train loss: 28.181919\n",
      "Validation loss: 29.332087 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 28.6817\n",
      "Step 200 / 547, loss = 25.1090\n",
      "Step 300 / 547, loss = 33.1638\n",
      "Step 400 / 547, loss = 22.4192\n",
      "Step 500 / 547, loss = 26.1284\n",
      "Train loss: 27.838474\n",
      "Validation loss: 32.620426 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 27.7872\n",
      "Step 200 / 547, loss = 27.4629\n",
      "Step 300 / 547, loss = 22.9507\n",
      "Step 400 / 547, loss = 25.2913\n",
      "Step 500 / 547, loss = 24.3590\n",
      "Train loss: 27.519777\n",
      "Validation loss: 73.164818 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 28.0681\n",
      "Step 200 / 547, loss = 23.6925\n",
      "Step 300 / 547, loss = 29.2713\n",
      "Step 400 / 547, loss = 25.8150\n",
      "Step 500 / 547, loss = 27.4693\n",
      "Train loss: 27.334257\n",
      "Validation loss: 36.550842 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 21\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 33.8850\n",
      "Step 200 / 547, loss = 35.9335\n",
      "Step 300 / 547, loss = 22.7057\n",
      "Step 400 / 547, loss = 28.3354\n",
      "Step 500 / 547, loss = 24.5943\n",
      "Train loss: 26.738670\n",
      "Validation loss: 34.283535 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 22\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 24.5769\n",
      "Step 200 / 547, loss = 29.7234\n",
      "Step 300 / 547, loss = 24.3669\n",
      "Step 400 / 547, loss = 24.7948\n",
      "Step 500 / 547, loss = 25.8128\n",
      "Train loss: 26.217524\n",
      "Validation loss: 31.375547 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 23\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 25.8008\n",
      "Step 200 / 547, loss = 25.0580\n",
      "Step 300 / 547, loss = 26.3974\n",
      "Step 400 / 547, loss = 27.4638\n",
      "Step 500 / 547, loss = 32.9044\n",
      "Train loss: 26.141878\n",
      "Validation loss: 29.194582 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 24\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 26.6869\n",
      "Step 200 / 547, loss = 25.9038\n",
      "Step 300 / 547, loss = 24.7367\n",
      "Step 400 / 547, loss = 22.3842\n",
      "Step 500 / 547, loss = 21.0640\n",
      "Train loss: 25.529099\n",
      "Validation loss: 143.883057 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 25\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 29.7391\n",
      "Step 200 / 547, loss = 21.6594\n",
      "Step 300 / 547, loss = 27.1341\n",
      "Step 400 / 547, loss = 27.0676\n",
      "Step 500 / 547, loss = 26.7319\n",
      "Train loss: 25.477331\n",
      "Validation loss: 41.447521 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 26\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 19.9368\n",
      "Step 200 / 547, loss = 27.4399\n",
      "Step 300 / 547, loss = 21.4116\n",
      "Step 400 / 547, loss = 21.4435\n",
      "Step 500 / 547, loss = 23.6114\n",
      "Train loss: 25.112909\n",
      "Validation loss: 31.261173 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 27\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.0533\n",
      "Step 200 / 547, loss = 26.0404\n",
      "Step 300 / 547, loss = 24.4156\n",
      "Step 400 / 547, loss = 27.6103\n",
      "Step 500 / 547, loss = 23.3398\n",
      "Train loss: 24.852556\n",
      "Validation loss: 27.973705 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 28\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.9053\n",
      "Step 200 / 547, loss = 32.9561\n",
      "Step 300 / 547, loss = 20.1843\n",
      "Step 400 / 547, loss = 30.9808\n",
      "Step 500 / 547, loss = 21.7523\n",
      "Train loss: 24.361937\n",
      "Validation loss: 103.066376 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 29\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 28.4493\n",
      "Step 200 / 547, loss = 26.0656\n",
      "Step 300 / 547, loss = 23.0898\n",
      "Step 400 / 547, loss = 22.2350\n",
      "Step 500 / 547, loss = 25.0971\n",
      "Train loss: 23.994246\n",
      "Validation loss: 31.348738 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 30\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.4735\n",
      "Step 200 / 547, loss = 21.6714\n",
      "Step 300 / 547, loss = 20.1977\n",
      "Step 400 / 547, loss = 25.6581\n",
      "Step 500 / 547, loss = 24.3444\n",
      "Train loss: 23.763828\n",
      "Validation loss: 33.725880 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 31\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 27.7733\n",
      "Step 200 / 547, loss = 26.9802\n",
      "Step 300 / 547, loss = 23.1881\n",
      "Step 400 / 547, loss = 21.2150\n",
      "Step 500 / 547, loss = 24.8549\n",
      "Train loss: 23.639189\n",
      "Validation loss: 144.219696 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 32\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 18.9737\n",
      "Step 200 / 547, loss = 19.2964\n",
      "Step 300 / 547, loss = 29.7643\n",
      "Step 400 / 547, loss = 22.5128\n",
      "Step 500 / 547, loss = 17.3435\n",
      "Train loss: 23.565189\n",
      "Validation loss: 71.533104 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 33\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 19.9962\n",
      "Step 200 / 547, loss = 19.5697\n",
      "Step 300 / 547, loss = 20.2061\n",
      "Step 400 / 547, loss = 26.0506\n",
      "Step 500 / 547, loss = 19.7652\n",
      "Train loss: 22.953323\n",
      "Validation loss: 55.896740 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 34\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 25.2083\n",
      "Step 200 / 547, loss = 16.9527\n",
      "Step 300 / 547, loss = 18.6339\n",
      "Step 400 / 547, loss = 21.5773\n",
      "Step 500 / 547, loss = 21.0022\n",
      "Train loss: 22.615683\n",
      "Validation loss: 61.563526 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 35\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 23.6322\n",
      "Step 200 / 547, loss = 22.6981\n",
      "Step 300 / 547, loss = 20.1094\n",
      "Step 400 / 547, loss = 22.4355\n",
      "Step 500 / 547, loss = 31.1381\n",
      "Train loss: 22.236620\n",
      "Validation loss: 38.583347 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 36\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 26.7418\n",
      "Step 200 / 547, loss = 21.6521\n",
      "Step 300 / 547, loss = 20.2058\n",
      "Step 400 / 547, loss = 24.2826\n",
      "Step 500 / 547, loss = 20.0883\n",
      "Train loss: 22.055927\n",
      "Validation loss: 62.358932 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 37\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 22.7691\n",
      "Step 200 / 547, loss = 24.8398\n",
      "Step 300 / 547, loss = 24.5493\n",
      "Step 400 / 547, loss = 24.0398\n",
      "Step 500 / 547, loss = 25.9988\n",
      "Train loss: 21.755100\n",
      "Validation loss: 68.951500 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.001\n",
      "Epoch 38\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 23.0219\n",
      "Step 200 / 547, loss = 21.0473\n",
      "Step 300 / 547, loss = 26.1701\n",
      "Step 400 / 547, loss = 21.7241\n",
      "Step 500 / 547, loss = 23.4925\n",
      "Train loss: 21.608192\n",
      "Validation loss: 62.550713 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 39\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.9385\n",
      "Step 200 / 547, loss = 20.7541\n",
      "Step 300 / 547, loss = 22.0097\n",
      "Step 400 / 547, loss = 20.6202\n",
      "Step 500 / 547, loss = 19.1740\n",
      "Train loss: 20.310812\n",
      "Validation loss: 28.857031 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 40\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.0035\n",
      "Step 200 / 547, loss = 17.8893\n",
      "Step 300 / 547, loss = 21.4935\n",
      "Step 400 / 547, loss = 22.5941\n",
      "Step 500 / 547, loss = 18.0782\n",
      "Train loss: 19.924068\n",
      "Validation loss: 29.737053 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 41\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.3388\n",
      "Step 200 / 547, loss = 17.7842\n",
      "Step 300 / 547, loss = 21.2881\n",
      "Step 400 / 547, loss = 19.8456\n",
      "Step 500 / 547, loss = 19.2312\n",
      "Train loss: 19.663887\n",
      "Validation loss: 60.016094 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 42\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.9607\n",
      "Step 200 / 547, loss = 17.3541\n",
      "Step 300 / 547, loss = 18.6001\n",
      "Step 400 / 547, loss = 17.1224\n",
      "Step 500 / 547, loss = 17.1190\n",
      "Train loss: 19.552734\n",
      "Validation loss: 35.160698 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 43\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 23.6357\n",
      "Step 200 / 547, loss = 17.7371\n",
      "Step 300 / 547, loss = 22.3564\n",
      "Step 400 / 547, loss = 21.0746\n",
      "Step 500 / 547, loss = 16.1246\n",
      "Train loss: 19.289640\n",
      "Validation loss: 35.089622 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 44\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 18.2061\n",
      "Step 200 / 547, loss = 27.7052\n",
      "Step 300 / 547, loss = 26.8132\n",
      "Step 400 / 547, loss = 15.4184\n",
      "Step 500 / 547, loss = 16.5917\n",
      "Train loss: 18.988703\n",
      "Validation loss: 30.047184 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 45\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 22.7189\n",
      "Step 200 / 547, loss = 18.1570\n",
      "Step 300 / 547, loss = 20.1453\n",
      "Step 400 / 547, loss = 13.0243\n",
      "Step 500 / 547, loss = 18.5114\n",
      "Train loss: 18.896900\n",
      "Validation loss: 38.964645 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 46\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.5185\n",
      "Step 200 / 547, loss = 16.1449\n",
      "Step 300 / 547, loss = 18.6952\n",
      "Step 400 / 547, loss = 17.1988\n",
      "Step 500 / 547, loss = 17.8080\n",
      "Train loss: 18.694159\n",
      "Validation loss: 49.629482 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 47\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 15.9399\n",
      "Step 200 / 547, loss = 19.3246\n",
      "Step 300 / 547, loss = 16.6402\n",
      "Step 400 / 547, loss = 15.5628\n",
      "Step 500 / 547, loss = 19.9162\n",
      "Train loss: 18.347845\n",
      "Validation loss: 66.911224 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 48\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.8786\n",
      "Step 200 / 547, loss = 16.2613\n",
      "Step 300 / 547, loss = 21.4862\n",
      "Step 400 / 547, loss = 23.8112\n",
      "Step 500 / 547, loss = 20.3069\n",
      "Train loss: 18.164303\n",
      "Validation loss: 32.808014 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0005\n",
      "Epoch 49\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.3528\n",
      "Step 200 / 547, loss = 18.4400\n",
      "Step 300 / 547, loss = 14.9101\n",
      "Step 400 / 547, loss = 15.9976\n",
      "Step 500 / 547, loss = 22.4249\n",
      "Train loss: 18.197145\n",
      "Validation loss: 33.711323 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 50\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 18.6331\n",
      "Step 200 / 547, loss = 14.6659\n",
      "Step 300 / 547, loss = 23.2017\n",
      "Step 400 / 547, loss = 14.9117\n",
      "Step 500 / 547, loss = 18.2047\n",
      "Train loss: 17.342764\n",
      "Validation loss: 60.272800 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 51\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.2764\n",
      "Step 200 / 547, loss = 16.4072\n",
      "Step 300 / 547, loss = 16.4817\n",
      "Step 400 / 547, loss = 15.0369\n",
      "Step 500 / 547, loss = 17.0707\n",
      "Train loss: 16.953674\n",
      "Validation loss: 76.314201 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 52\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 18.2559\n",
      "Step 200 / 547, loss = 20.9873\n",
      "Step 300 / 547, loss = 21.2072\n",
      "Step 400 / 547, loss = 25.4024\n",
      "Step 500 / 547, loss = 25.7278\n",
      "Train loss: 16.806152\n",
      "Validation loss: 34.719830 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 53\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 11.7744\n",
      "Step 200 / 547, loss = 20.1465\n",
      "Step 300 / 547, loss = 16.4502\n",
      "Step 400 / 547, loss = 17.5234\n",
      "Step 500 / 547, loss = 20.2221\n",
      "Train loss: 16.714256\n",
      "Validation loss: 32.863514 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 54\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.3273\n",
      "Step 200 / 547, loss = 18.5187\n",
      "Step 300 / 547, loss = 17.0360\n",
      "Step 400 / 547, loss = 15.8944\n",
      "Step 500 / 547, loss = 14.2022\n",
      "Train loss: 16.783321\n",
      "Validation loss: 40.237568 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 55\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.9572\n",
      "Step 200 / 547, loss = 15.2150\n",
      "Step 300 / 547, loss = 17.6769\n",
      "Step 400 / 547, loss = 16.5556\n",
      "Step 500 / 547, loss = 12.4463\n",
      "Train loss: 16.422235\n",
      "Validation loss: 66.900772 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 56\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.1372\n",
      "Step 200 / 547, loss = 17.5540\n",
      "Step 300 / 547, loss = 19.6294\n",
      "Step 400 / 547, loss = 19.3612\n",
      "Step 500 / 547, loss = 15.9945\n",
      "Train loss: 16.360615\n",
      "Validation loss: 81.304024 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 57\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 18.0947\n",
      "Step 200 / 547, loss = 16.5508\n",
      "Step 300 / 547, loss = 14.2669\n",
      "Step 400 / 547, loss = 17.3340\n",
      "Step 500 / 547, loss = 20.4064\n",
      "Train loss: 16.341452\n",
      "Validation loss: 32.815033 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 58\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 12.9928\n",
      "Step 200 / 547, loss = 16.8177\n",
      "Step 300 / 547, loss = 15.6140\n",
      "Step 400 / 547, loss = 18.7288\n",
      "Step 500 / 547, loss = 12.1876\n",
      "Train loss: 16.203873\n",
      "Validation loss: 40.402077 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 59\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.4753\n",
      "Step 200 / 547, loss = 15.4488\n",
      "Step 300 / 547, loss = 15.3880\n",
      "Step 400 / 547, loss = 19.3557\n",
      "Step 500 / 547, loss = 17.6438\n",
      "Train loss: 16.004726\n",
      "Validation loss: 40.010971 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00025\n",
      "Epoch 60\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 13.3075\n",
      "Step 200 / 547, loss = 13.8716\n",
      "Step 300 / 547, loss = 19.1995\n",
      "Step 400 / 547, loss = 15.0937\n",
      "Step 500 / 547, loss = 23.3873\n",
      "Train loss: 16.109245\n",
      "Validation loss: 35.583588 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.000125\n",
      "Epoch 61\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.1134\n",
      "Step 200 / 547, loss = 15.5471\n",
      "Step 300 / 547, loss = 13.5614\n",
      "Step 400 / 547, loss = 23.7548\n",
      "Step 500 / 547, loss = 17.9761\n",
      "Train loss: 15.452288\n",
      "Validation loss: 37.885651 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.000125\n",
      "Epoch 62\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 14.7274\n",
      "Step 200 / 547, loss = 15.6713\n",
      "Step 300 / 547, loss = 15.6999\n",
      "Step 400 / 547, loss = 16.6162\n",
      "Step 500 / 547, loss = 10.1977\n",
      "Train loss: 15.446085\n",
      "Validation loss: 35.541340 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.000125\n",
      "Epoch 63\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 18.8338\n",
      "Step 200 / 547, loss = 14.0277\n",
      "Step 300 / 547, loss = 16.5355\n",
      "Step 400 / 547, loss = 16.6295\n",
      "Step 500 / 547, loss = 20.3310\n",
      "Train loss: 15.213873\n",
      "Validation loss: 32.303192 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.000125\n",
      "Epoch 64\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.1336\n",
      "Step 200 / 547, loss = 14.1308\n",
      "Step 300 / 547, loss = 14.5287\n",
      "Step 400 / 547, loss = 19.8857\n",
      "Step 500 / 547, loss = 18.6304\n",
      "Train loss: 15.308798\n",
      "Validation loss: 43.567951 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.000125\n",
      "Epoch 65\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 12.0935\n",
      "Step 200 / 547, loss = 14.5013\n",
      "Step 300 / 547, loss = 16.6043\n",
      "Step 400 / 547, loss = 15.5402\n",
      "Step 500 / 547, loss = 15.0092\n",
      "Train loss: 15.241978\n",
      "Validation loss: 42.177155 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.000125\n",
      "Epoch 66\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 13.5966\n",
      "Step 200 / 547, loss = 12.9174\n",
      "Step 300 / 547, loss = 17.7472\n",
      "Step 400 / 547, loss = 14.2236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\17F2~1\\AppData\\Local\\Temp/ipykernel_1996/3484931062.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"lr: {optimizer.param_groups[0]['lr']}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch+1}\\n--------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\17F2~1\\AppData\\Local\\Temp/ipykernel_1996/1742191283.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;31m# scheduler.step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             F.adamw(params_with_grad,\n\u001b[0m\u001b[0;32m    138\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # if epoch % 3 == 0:\n",
    "    #     if low_lr:\n",
    "    #         actual_lr *= 0.8\n",
    "    #         optimizer.param_groups[0]['lr'] = actual_lr\n",
    "    #     else:\n",
    "    #         optimizer.param_groups[0]['lr'] = lr_min\n",
    "    #     low_lr = not low_lr\n",
    "    \n",
    "    print(f\"lr: {optimizer.param_groups[0]['lr']}\")\n",
    "    print(f'Epoch {epoch+1}\\n--------------------------')\n",
    "    loss = train(train_dataloader, model, criterion, optimizer)\n",
    "    train_epoch_loss[epoch] = loss\n",
    "    loss = test(valid_dataloader, model, criterion)\n",
    "    val_epoch_loss[epoch] = loss\n",
    "    scheduler.step(loss)\n",
    "    # scheduler.step()\n",
    "    checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint, f'{epoch}.pth.tar')\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-6]\n",
    "while learning_rates[-1] < 1:\n",
    "    learning_rates.append(learning_rates[-1] * 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_lr_loss = np.zeros(len(learning_rates))\n",
    "n_epochs = 3\n",
    "for (i, lr) in enumerate(learning_rates):\n",
    "    print(f\"lr = {lr}\")\n",
    "    model = NeuralNetwork().to(device)\n",
    "    criterion = mean_absolute_percentage_error\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    # optimizer.param_groups[0]['lr'] = lr\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch+1}\\n--------------------------')\n",
    "        loss = train(train_dataloader, model, criterion, optimizer)\n",
    "        train_epoch_loss[epoch] = loss\n",
    "        epoch_lr_loss[i] = loss\n",
    "        # loss = test(valid_dataloader, model, criterion)\n",
    "        # val_epoch_loss[epoch] = loss\n",
    "        # checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "        # save_checkpoint(checkpoint, f'{epoch}.pth.tar')\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(learning_rates) + 1), epoch_lr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_boundary = np.array(epoch_lr_loss).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learning_rates[lr_boundary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max = learning_rates[lr_boundary] / 3\n",
    "lr_min = lr_max / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = val_epoch_loss.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 67\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(imputed_test)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, shuffle=False)\n",
    "best_model = val_epoch_loss.argmin()\n",
    "print(\"Best epoch:\", best_model)\n",
    "load_checkpoint(torch.load(f'{best_model}.pth.tar'))\n",
    "\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for x in test_dataloader:\n",
    "        x = x.to(device)\n",
    "        pred = model(x)\n",
    "        y_pred.append(pred.item())\n",
    "\n",
    "y_pred = pd.DataFrame(data={\n",
    "    'id': indexes, \n",
    "    'Predicted': y_pred}, index=indexes)\n",
    "\n",
    "pd.DataFrame(y_pred).to_csv('submission.csv',\n",
    "        columns=['id', 'Predicted'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x183221bd160>,\n",
       " <matplotlib.lines.Line2D at 0x183221bd190>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, n_epo + 1), train_epoch_loss[:cnt], 'g', range(1, cnt + 1), val_epoch_loss[:cnt], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.3481"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_epoch_loss[best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestRegressor(n_estimators=1000)\n",
    "rf_clf.fit(imputed_train_X, train_y)\n",
    "y_pred = rf_clf.predict(imputed_val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_pred, torch.tensor(np.array(val_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
