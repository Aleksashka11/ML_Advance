{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 28\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# train/val\n",
    "train_data = pd.read_csv('ttids21/train.csv')\n",
    "zipcode_data = pd.read_csv('ttids21/zipcodes.csv')\n",
    "zipcode_data.drop_duplicates(subset=['zipcode'], keep='first', inplace=True)\n",
    "\n",
    "X = train_data.drop(columns=['Unnamed: 0', 'price'])\n",
    "y = train_data.price\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.7, random_state=SEED)\n",
    "\n",
    "def f(x):\n",
    "    if x < 20:\n",
    "        return 2000 + x\n",
    "    if x < 100:\n",
    "        return 1900 + x\n",
    "    return x\n",
    "\n",
    "train_X.registration_year = train_X.registration_year.map(f)\n",
    "val_X.registration_year = val_X.registration_year.map(f)\n",
    "\n",
    "train_X = pd.merge(train_X, zipcode_data, left_on='zipcode', right_on='zipcode', how=\"left\")\n",
    "val_X = pd.merge(val_X, zipcode_data, left_on='zipcode', right_on='zipcode', how=\"left\")\n",
    "\n",
    "my_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "imputed_train_X = pd.DataFrame(my_imputer.fit_transform(train_X))\n",
    "imputed_val_X = pd.DataFrame(my_imputer.transform(val_X))\n",
    "\n",
    "imputed_train_X.columns = train_X.columns\n",
    "imputed_val_X.columns = val_X.columns\n",
    "\n",
    "clustering_data = imputed_train_X[['latitude', 'longitude']]\n",
    "cluster_picker = KMeans(n_clusters=200)\n",
    "clusters = cluster_picker.fit_predict(clustering_data)\n",
    "imputed_train_X['geo_cluster'] = clusters\n",
    "clustering_data = imputed_val_X[['latitude', 'longitude']]\n",
    "imputed_val_X['geo_cluster'] = cluster_picker.predict(clustering_data)\n",
    "\n",
    "imputed_train_X.drop(columns=['Unnamed: 0', 'zipcode', 'city', 'latitude', 'longitude'])\n",
    "imputed_val_X.drop(columns=['Unnamed: 0', 'zipcode', 'city', 'latitude', 'longitude'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "columns_to_transform = ['type', 'model', 'fuel', 'brand']\n",
    "\n",
    "d = dict()\n",
    "label_encoders = dict()\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(np.append(imputed_train_X[col], ['-1']))\n",
    "    label_encoders[col] = le\n",
    "    imputed_train_X[col] = le.transform(imputed_train_X[col])\n",
    "    classes = le.classes_\n",
    "    d[col] = len(le.classes_)\n",
    "    unknown_classes = set(imputed_val_X[col]) - set(classes)\n",
    "    imputed_val_X[col].replace(unknown_classes, '-1', inplace=True)\n",
    "    imputed_val_X[col] = le.transform(imputed_val_X[col])\n",
    "\n",
    "imputed_train_X.replace(['manual', 'auto'], [0, 1], inplace=True)\n",
    "imputed_val_X.replace(['manual', 'auto'], [0, 1], inplace=True)\n",
    "\n",
    "columns_to_remove = ['zipcode', 'Unnamed: 0', 'city', 'latitude', 'longitude']\n",
    "\n",
    "imputed_train_X.drop(columns=columns_to_remove, inplace=True)\n",
    "imputed_val_X.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# test\n",
    "test_data = pd.read_csv('ttids21/test_no_target.csv')\n",
    "\n",
    "test_data.index = test_data['Unnamed: 0']\n",
    "indexes = test_data.index\n",
    "\n",
    "test_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "test_data.registration_year = test_data.registration_year.map(f)\n",
    "\n",
    "test_data = pd.merge(test_data, zipcode_data, left_on='zipcode', right_on='zipcode', how=\"left\")\n",
    "test_data.index = indexes\n",
    "\n",
    "imputed_test = pd.DataFrame(my_imputer.transform(test_data))\n",
    "imputed_test.columns = test_data.columns\n",
    "imputed_test.index = indexes\n",
    "\n",
    "clustering_data = imputed_test[['latitude', 'longitude']]\n",
    "imputed_test['geo_cluster'] = cluster_picker.predict(clustering_data)\n",
    "imputed_test.drop(columns=['Unnamed: 0', 'zipcode', 'city', 'latitude', 'longitude'])\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    le = label_encoders[col]\n",
    "    classes = le.classes_\n",
    "    unknown_classes = set(imputed_test[col]) - set(classes)\n",
    "    imputed_test[col].replace(unknown_classes, '-1', inplace=True)\n",
    "    imputed_test[col] = le.transform(imputed_test[col])\n",
    "\n",
    "imputed_test.replace(['manual', 'auto'], [0, 1], inplace=True)\n",
    "imputed_test.drop(columns=columns_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17F2~1\\AppData\\Local\\Temp/ipykernel_1996/3457423260.py:32: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  torch.nn.init.kaiming_uniform(m.weight)\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_samples = x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x.iloc[index].values, self.y.iloc[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        self.n_samples = x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x.iloc[index].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# n_types = imputed_train_X.type.unique()\n",
    "# n_models = imputed_train_X.model.unique()\n",
    "# n_fuels = imputed_train_X.fuel.unique()\n",
    "# n_brands = imputed_train_X.type.unique()\n",
    "n_geo_points = cluster_picker.n_clusters\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "size1 = 256\n",
    "size2 = 1024\n",
    "size3 = 6*size1 + size2\n",
    "size4 = 2048\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.binary_branch = nn.Sequential(\n",
    "            nn.Linear(2, size1),\n",
    "            nn.BatchNorm1d(num_features=size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            nn.BatchNorm1d(num_features=size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.features_branch = nn.Sequential(\n",
    "            nn.Linear(5, size2),\n",
    "            nn.BatchNorm1d(num_features=size2),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size2, size2),\n",
    "            nn.BatchNorm1d(num_features=size2),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size2, size2),\n",
    "            nn.BatchNorm1d(num_features=size2),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.type_branch = nn.Sequential(\n",
    "            nn.Embedding(d['type'], size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.model_branch = nn.Sequential(\n",
    "            nn.Embedding(d['model'], size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fuel_branch = nn.Sequential(\n",
    "            nn.Embedding(d['fuel'], size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.brand_branch = nn.Sequential(\n",
    "            nn.Embedding(d['brand'], size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.geo_branch = nn.Sequential(\n",
    "            nn.Embedding(n_geo_points, size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size1, size1),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.main_branch = nn.Sequential(\n",
    "            nn.Linear(size3, size4),\n",
    "            nn.BatchNorm1d(num_features=size4),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size4, size4),\n",
    "            nn.BatchNorm1d(num_features=size4),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size4, size4),\n",
    "            nn.BatchNorm1d(num_features=size4),\n",
    "            # nn.Dropout(p=0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size4, 1)\n",
    "        )\n",
    "        \n",
    "        self.binary_branch.apply(init_weights)\n",
    "        self.features_branch.apply(init_weights)\n",
    "        self.type_branch.apply(init_weights)\n",
    "        self.model_branch.apply(init_weights)\n",
    "        self.fuel_branch.apply(init_weights)\n",
    "        self.brand_branch.apply(init_weights)\n",
    "        self.geo_branch.apply(init_weights)\n",
    "        self.main_branch.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.binary_branch(x[:, [3, 9]].to(torch.float32))\n",
    "        out2 = self.features_branch(x[:, [0, 2, 4, 6, 10]].to(torch.float32))\n",
    "        out3 = self.type_branch(x[:, 1].to(torch.long))\n",
    "        out4 = self.model_branch(x[:, 5].to(torch.long))\n",
    "        out5 = self.fuel_branch(x[:, 7].to(torch.long))\n",
    "        out6 = self.brand_branch(x[:, 8].to(torch.long))\n",
    "        out7 = self.geo_branch(x[:, 11].to(torch.long))\n",
    "        out = torch.cat((out1, out2, out3, out4, out5, out6, out7), 1)\n",
    "        out = self.main_branch(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "def mean_absolute_percentage_error(y_pred, y_true):\n",
    "    return torch.mean(torch.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(imputed_train_X, train_y)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = CustomDataset(imputed_val_X, val_y)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "n_epochs = 100\n",
    "train_epoch_loss = np.zeros(n_epochs, dtype=np.float32)\n",
    "val_epoch_loss = np.zeros(n_epochs, dtype=np.float32)\n",
    "criterion = mean_absolute_percentage_error\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, nesterov=True, weight_decay= 0.0001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=0.000001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 5)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.00005, max_lr=0.002, mode='exp_range', gamma=0.99994, step_size_up=len(train_dataloader)*2, cycle_momentum=False)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr_max, steps_per_epoch=len(train_dataloader), epochs=n_epochs)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "# actual_lr = lr_max\n",
    "# low_lr = True\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch=0):\n",
    "    n_iterations = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    y_pred = torch.tensor([]).to(device)\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y = y.reshape(y.shape[0], 1).type(torch.float32)\n",
    "\n",
    "        pred = model(x)\n",
    "        pred = pred.to(device)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        y_pred = torch.cat((y_pred, pred))\n",
    "        y_true = torch.cat((y_true, y))\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + i / n_iterations)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Step {i+1} / {n_iterations}, loss = {loss.item():.4f}')\n",
    "    \n",
    "    train_loss = loss_fn(y_pred, y_true)\n",
    "    print(f'Train loss: {train_loss:>8f}')\n",
    "    return train_loss\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y_pred = torch.tensor([]).to(device)\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y = y.reshape(y.shape[0], 1).to(torch.float32)\n",
    "            \n",
    "            pred = model(x)\n",
    "            pred = pred.to(device)\n",
    "            y_pred = torch.cat((y_pred, pred))\n",
    "            y_true = torch.cat((y_true, y))\n",
    "\n",
    "    # y_pred = torch.tensor(y_pred).to(device)\n",
    "    # y_true = torch.tensor(y_true).to(device)\n",
    "    test_loss = loss_fn(y_pred, y_true)\n",
    "    print(f'Validation loss: {test_loss:>8f} \\n')\n",
    "    return test_loss\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    print(\"Saving checkpoint...\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint):\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-6]\n",
    "while learning_rates[-1] < 1:\n",
    "    learning_rates.append(learning_rates[-1] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-06,\n",
       " 2e-06,\n",
       " 4e-06,\n",
       " 8e-06,\n",
       " 1.6e-05,\n",
       " 3.2e-05,\n",
       " 6.4e-05,\n",
       " 0.000128,\n",
       " 0.000256,\n",
       " 0.000512,\n",
       " 0.001024,\n",
       " 0.002048,\n",
       " 0.004096,\n",
       " 0.008192,\n",
       " 0.016384,\n",
       " 0.032768,\n",
       " 0.065536,\n",
       " 0.131072,\n",
       " 0.262144,\n",
       " 0.524288,\n",
       " 1.048576]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\17F2~1\\AppData\\Local\\Temp/ipykernel_10840/1033684717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlr_max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_max' is not defined"
     ]
    }
   ],
   "source": [
    "lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.002\n",
      "Epoch 1\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 77.1937\n",
      "Step 200 / 547, loss = 74.1237\n",
      "Step 300 / 547, loss = 64.3988\n",
      "Step 400 / 547, loss = 54.4836\n",
      "Step 500 / 547, loss = 59.0192\n",
      "Train loss: 67.088737\n",
      "Validation loss: 97.847221 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387068\n",
      "Epoch 2\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 48.3169\n",
      "Step 200 / 547, loss = 48.1383\n",
      "Step 300 / 547, loss = 43.9378\n",
      "Step 400 / 547, loss = 41.8830\n",
      "Step 500 / 547, loss = 45.7933\n",
      "Train loss: 44.922764\n",
      "Validation loss: 38.133183 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768453\n",
      "Epoch 3\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 39.0543\n",
      "Step 200 / 547, loss = 46.6485\n",
      "Step 300 / 547, loss = 42.7640\n",
      "Step 400 / 547, loss = 33.3327\n",
      "Step 500 / 547, loss = 32.3065\n",
      "Train loss: 37.379284\n",
      "Validation loss: 44.661274 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520058\n",
      "Epoch 4\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 37.4070\n",
      "Step 200 / 547, loss = 33.6308\n",
      "Step 300 / 547, loss = 31.5095\n",
      "Step 400 / 547, loss = 33.8834\n",
      "Step 500 / 547, loss = 32.1420\n",
      "Train loss: 34.096405\n",
      "Validation loss: 36.326820 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686487\n",
      "Epoch 5\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 30.2921\n",
      "Step 200 / 547, loss = 29.9548\n",
      "Step 300 / 547, loss = 35.8202\n",
      "Step 400 / 547, loss = 30.9008\n",
      "Step 500 / 547, loss = 30.8876\n",
      "Train loss: 32.115986\n",
      "Validation loss: 36.418087 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 6\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 36.7997\n",
      "Step 200 / 547, loss = 28.6713\n",
      "Step 300 / 547, loss = 34.6134\n",
      "Step 400 / 547, loss = 36.9300\n",
      "Step 500 / 547, loss = 29.3208\n",
      "Train loss: 33.203625\n",
      "Validation loss: 34.075077 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387068\n",
      "Epoch 7\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 30.4462\n",
      "Step 200 / 547, loss = 31.7097\n",
      "Step 300 / 547, loss = 30.1476\n",
      "Step 400 / 547, loss = 31.2930\n",
      "Step 500 / 547, loss = 35.1676\n",
      "Train loss: 31.345850\n",
      "Validation loss: 32.187111 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768453\n",
      "Epoch 8\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 32.0383\n",
      "Step 200 / 547, loss = 28.3493\n",
      "Step 300 / 547, loss = 30.6561\n",
      "Step 400 / 547, loss = 30.1391\n",
      "Step 500 / 547, loss = 27.5349\n",
      "Train loss: 30.403614\n",
      "Validation loss: 40.267815 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520058\n",
      "Epoch 9\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 27.8898\n",
      "Step 200 / 547, loss = 27.1063\n",
      "Step 300 / 547, loss = 30.4334\n",
      "Step 400 / 547, loss = 23.1193\n",
      "Step 500 / 547, loss = 31.5573\n",
      "Train loss: 29.563240\n",
      "Validation loss: 30.648073 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686487\n",
      "Epoch 10\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 28.8345\n",
      "Step 200 / 547, loss = 29.5129\n",
      "Step 300 / 547, loss = 31.3438\n",
      "Step 400 / 547, loss = 34.1391\n",
      "Step 500 / 547, loss = 28.4225\n",
      "Train loss: 28.712475\n",
      "Validation loss: 28.874565 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 11\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 31.3695\n",
      "Step 200 / 547, loss = 31.3603\n",
      "Step 300 / 547, loss = 30.3645\n",
      "Step 400 / 547, loss = 41.2729\n",
      "Step 500 / 547, loss = 26.4172\n",
      "Train loss: 30.579529\n",
      "Validation loss: 49.553104 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387068\n",
      "Epoch 12\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 44.0970\n",
      "Step 200 / 547, loss = 23.5699\n",
      "Step 300 / 547, loss = 33.6373\n",
      "Step 400 / 547, loss = 30.1208\n",
      "Step 500 / 547, loss = 25.4677\n",
      "Train loss: 29.997540\n",
      "Validation loss: 34.017471 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768453\n",
      "Epoch 13\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 27.0562\n",
      "Step 200 / 547, loss = 25.5436\n",
      "Step 300 / 547, loss = 26.6263\n",
      "Step 400 / 547, loss = 30.8622\n",
      "Step 500 / 547, loss = 26.0041\n",
      "Train loss: 28.914673\n",
      "Validation loss: 65.991119 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520058\n",
      "Epoch 14\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 26.6137\n",
      "Step 200 / 547, loss = 26.4583\n",
      "Step 300 / 547, loss = 27.7835\n",
      "Step 400 / 547, loss = 27.1315\n",
      "Step 500 / 547, loss = 32.1054\n",
      "Train loss: 28.137976\n",
      "Validation loss: 29.558796 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686487\n",
      "Epoch 15\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 28.6478\n",
      "Step 200 / 547, loss = 29.4450\n",
      "Step 300 / 547, loss = 26.1772\n",
      "Step 400 / 547, loss = 33.7365\n",
      "Step 500 / 547, loss = 26.8994\n",
      "Train loss: 27.396070\n",
      "Validation loss: 29.610405 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 16\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 27.3601\n",
      "Step 200 / 547, loss = 27.8742\n",
      "Step 300 / 547, loss = 37.8422\n",
      "Step 400 / 547, loss = 26.9827\n",
      "Step 500 / 547, loss = 26.7130\n",
      "Train loss: 29.381779\n",
      "Validation loss: 32.948990 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387068\n",
      "Epoch 17\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 25.3349\n",
      "Step 200 / 547, loss = 29.5163\n",
      "Step 300 / 547, loss = 25.3942\n",
      "Step 400 / 547, loss = 33.2895\n",
      "Step 500 / 547, loss = 29.5440\n",
      "Train loss: 29.339382\n",
      "Validation loss: 161.331253 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 18\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 26.3119\n",
      "Step 200 / 547, loss = 25.5058\n",
      "Step 300 / 547, loss = 23.0643\n",
      "Step 400 / 547, loss = 32.0309\n",
      "Step 500 / 547, loss = 32.6297\n",
      "Train loss: 28.069651\n",
      "Validation loss: 31.618950 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 19\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 25.8162\n",
      "Step 200 / 547, loss = 28.4301\n",
      "Step 300 / 547, loss = 29.4480\n",
      "Step 400 / 547, loss = 29.6978\n",
      "Step 500 / 547, loss = 28.8043\n",
      "Train loss: 27.056778\n",
      "Validation loss: 31.773849 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 20\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 27.2435\n",
      "Step 200 / 547, loss = 24.0523\n",
      "Step 300 / 547, loss = 25.3754\n",
      "Step 400 / 547, loss = 29.9343\n",
      "Step 500 / 547, loss = 28.5267\n",
      "Train loss: 26.427784\n",
      "Validation loss: 30.931276 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 21\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 22.3725\n",
      "Step 200 / 547, loss = 27.4638\n",
      "Step 300 / 547, loss = 26.2749\n",
      "Step 400 / 547, loss = 24.8418\n",
      "Step 500 / 547, loss = 30.3729\n",
      "Train loss: 28.187912\n",
      "Validation loss: 33.567188 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 22\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 28.2339\n",
      "Step 200 / 547, loss = 25.6728\n",
      "Step 300 / 547, loss = 29.4238\n",
      "Step 400 / 547, loss = 24.9879\n",
      "Step 500 / 547, loss = 32.8548\n",
      "Train loss: 27.756088\n",
      "Validation loss: 76.763847 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 23\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 28.9065\n",
      "Step 200 / 547, loss = 35.5475\n",
      "Step 300 / 547, loss = 29.1252\n",
      "Step 400 / 547, loss = 23.4598\n",
      "Step 500 / 547, loss = 26.2413\n",
      "Train loss: 26.810425\n",
      "Validation loss: 38.715240 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 24\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 21.4244\n",
      "Step 200 / 547, loss = 26.3746\n",
      "Step 300 / 547, loss = 24.9741\n",
      "Step 400 / 547, loss = 21.9947\n",
      "Step 500 / 547, loss = 19.7033\n",
      "Train loss: 25.627577\n",
      "Validation loss: 26.946487 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 25\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 26.3666\n",
      "Step 200 / 547, loss = 27.6472\n",
      "Step 300 / 547, loss = 24.0826\n",
      "Step 400 / 547, loss = 27.2582\n",
      "Step 500 / 547, loss = 29.1019\n",
      "Train loss: 24.929174\n",
      "Validation loss: 79.746002 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 26\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 28.6413\n",
      "Step 200 / 547, loss = 27.1112\n",
      "Step 300 / 547, loss = 26.7282\n",
      "Step 400 / 547, loss = 29.3984\n",
      "Step 500 / 547, loss = 26.9249\n",
      "Train loss: 27.428898\n",
      "Validation loss: 98.302902 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 27\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 27.2241\n",
      "Step 200 / 547, loss = 25.0441\n",
      "Step 300 / 547, loss = 23.3518\n",
      "Step 400 / 547, loss = 25.1886\n",
      "Step 500 / 547, loss = 32.2070\n",
      "Train loss: 26.831085\n",
      "Validation loss: 42.908550 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 28\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 24.3082\n",
      "Step 200 / 547, loss = 21.3167\n",
      "Step 300 / 547, loss = 27.5981\n",
      "Step 400 / 547, loss = 27.3029\n",
      "Step 500 / 547, loss = 24.7402\n",
      "Train loss: 25.690456\n",
      "Validation loss: 34.899628 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 29\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 25.3080\n",
      "Step 200 / 547, loss = 20.7472\n",
      "Step 300 / 547, loss = 21.8791\n",
      "Step 400 / 547, loss = 19.9853\n",
      "Step 500 / 547, loss = 24.6345\n",
      "Train loss: 24.549208\n",
      "Validation loss: 36.799225 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 30\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 24.2317\n",
      "Step 200 / 547, loss = 18.0573\n",
      "Step 300 / 547, loss = 20.5646\n",
      "Step 400 / 547, loss = 19.2078\n",
      "Step 500 / 547, loss = 22.4199\n",
      "Train loss: 23.844364\n",
      "Validation loss: 29.288122 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 31\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 27.1241\n",
      "Step 200 / 547, loss = 25.3823\n",
      "Step 300 / 547, loss = 36.8575\n",
      "Step 400 / 547, loss = 28.7212\n",
      "Step 500 / 547, loss = 26.5667\n",
      "Train loss: 26.147007\n",
      "Validation loss: 36.622021 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 32\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 24.1935\n",
      "Step 200 / 547, loss = 28.1164\n",
      "Step 300 / 547, loss = 25.3734\n",
      "Step 400 / 547, loss = 26.2234\n",
      "Step 500 / 547, loss = 22.6755\n",
      "Train loss: 25.532488\n",
      "Validation loss: 32.208057 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 33\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 30.2268\n",
      "Step 200 / 547, loss = 23.1900\n",
      "Step 300 / 547, loss = 22.8667\n",
      "Step 400 / 547, loss = 26.7480\n",
      "Step 500 / 547, loss = 23.9722\n",
      "Train loss: 24.511808\n",
      "Validation loss: 70.205666 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 34\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 24.2981\n",
      "Step 200 / 547, loss = 23.9593\n",
      "Step 300 / 547, loss = 23.2679\n",
      "Step 400 / 547, loss = 24.7057\n",
      "Step 500 / 547, loss = 28.2358\n",
      "Train loss: 23.489990\n",
      "Validation loss: 29.419426 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 35\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.3397\n",
      "Step 200 / 547, loss = 25.0320\n",
      "Step 300 / 547, loss = 20.1801\n",
      "Step 400 / 547, loss = 18.2203\n",
      "Step 500 / 547, loss = 20.2587\n",
      "Train loss: 22.845083\n",
      "Validation loss: 27.914679 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 36\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 24.6378\n",
      "Step 200 / 547, loss = 23.8089\n",
      "Step 300 / 547, loss = 28.3125\n",
      "Step 400 / 547, loss = 23.5596\n",
      "Step 500 / 547, loss = 24.0248\n",
      "Train loss: 25.399796\n",
      "Validation loss: 43.002911 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 37\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 22.2483\n",
      "Step 200 / 547, loss = 22.8319\n",
      "Step 300 / 547, loss = 26.6202\n",
      "Step 400 / 547, loss = 25.5102\n",
      "Step 500 / 547, loss = 21.7479\n",
      "Train loss: 24.841530\n",
      "Validation loss: 37.094269 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 38\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 22.4896\n",
      "Step 200 / 547, loss = 21.9709\n",
      "Step 300 / 547, loss = 25.5532\n",
      "Step 400 / 547, loss = 20.3604\n",
      "Step 500 / 547, loss = 18.9025\n",
      "Train loss: 23.562458\n",
      "Validation loss: 33.776386 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 39\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 24.2782\n",
      "Step 200 / 547, loss = 25.5093\n",
      "Step 300 / 547, loss = 21.7695\n",
      "Step 400 / 547, loss = 19.1605\n",
      "Step 500 / 547, loss = 20.7891\n",
      "Train loss: 22.577282\n",
      "Validation loss: 28.252687 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 40\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 19.0827\n",
      "Step 200 / 547, loss = 25.4940\n",
      "Step 300 / 547, loss = 21.9860\n",
      "Step 400 / 547, loss = 29.6564\n",
      "Step 500 / 547, loss = 21.1236\n",
      "Train loss: 21.779324\n",
      "Validation loss: 27.514013 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 41\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 19.4744\n",
      "Step 200 / 547, loss = 22.6516\n",
      "Step 300 / 547, loss = 19.5868\n",
      "Step 400 / 547, loss = 21.9595\n",
      "Step 500 / 547, loss = 25.1535\n",
      "Train loss: 24.506149\n",
      "Validation loss: 31.071789 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 42\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 21.9899\n",
      "Step 200 / 547, loss = 24.3566\n",
      "Step 300 / 547, loss = 22.4061\n",
      "Step 400 / 547, loss = 25.4086\n",
      "Step 500 / 547, loss = 25.9679\n",
      "Train loss: 23.966835\n",
      "Validation loss: 37.119049 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 43\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 23.0075\n",
      "Step 200 / 547, loss = 18.2709\n",
      "Step 300 / 547, loss = 24.3278\n",
      "Step 400 / 547, loss = 24.5634\n",
      "Step 500 / 547, loss = 22.2333\n",
      "Train loss: 22.614355\n",
      "Validation loss: 39.134789 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 44\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.9322\n",
      "Step 200 / 547, loss = 23.3217\n",
      "Step 300 / 547, loss = 22.4622\n",
      "Step 400 / 547, loss = 20.8759\n",
      "Step 500 / 547, loss = 21.5508\n",
      "Train loss: 21.334024\n",
      "Validation loss: 28.841543 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 45\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 21.3712\n",
      "Step 200 / 547, loss = 17.6847\n",
      "Step 300 / 547, loss = 18.6252\n",
      "Step 400 / 547, loss = 19.1080\n",
      "Step 500 / 547, loss = 23.9941\n",
      "Train loss: 20.746517\n",
      "Validation loss: 29.076237 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 46\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 21.3215\n",
      "Step 200 / 547, loss = 21.8145\n",
      "Step 300 / 547, loss = 22.1473\n",
      "Step 400 / 547, loss = 21.8410\n",
      "Step 500 / 547, loss = 19.0900\n",
      "Train loss: 23.292027\n",
      "Validation loss: 100.379219 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 47\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 25.4249\n",
      "Step 200 / 547, loss = 22.4247\n",
      "Step 300 / 547, loss = 23.1094\n",
      "Step 400 / 547, loss = 21.4585\n",
      "Step 500 / 547, loss = 17.7008\n",
      "Train loss: 22.833761\n",
      "Validation loss: 76.011879 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 48\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 19.9307\n",
      "Step 200 / 547, loss = 19.1060\n",
      "Step 300 / 547, loss = 18.6167\n",
      "Step 400 / 547, loss = 26.1006\n",
      "Step 500 / 547, loss = 20.6735\n",
      "Train loss: 21.528254\n",
      "Validation loss: 33.622391 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 49\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.3803\n",
      "Step 200 / 547, loss = 23.9965\n",
      "Step 300 / 547, loss = 21.4016\n",
      "Step 400 / 547, loss = 22.4543\n",
      "Step 500 / 547, loss = 20.2180\n",
      "Train loss: 20.557537\n",
      "Validation loss: 31.139812 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 50\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 19.6393\n",
      "Step 200 / 547, loss = 22.5954\n",
      "Step 300 / 547, loss = 17.2614\n",
      "Step 400 / 547, loss = 21.8825\n",
      "Step 500 / 547, loss = 17.7386\n",
      "Train loss: 19.568155\n",
      "Validation loss: 34.809132 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 51\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.8229\n",
      "Step 200 / 547, loss = 25.0205\n",
      "Step 300 / 547, loss = 25.0656\n",
      "Step 400 / 547, loss = 25.0097\n",
      "Step 500 / 547, loss = 24.0044\n",
      "Train loss: 22.744543\n",
      "Validation loss: 28.253143 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 52\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 21.9350\n",
      "Step 200 / 547, loss = 22.2963\n",
      "Step 300 / 547, loss = 21.3589\n",
      "Step 400 / 547, loss = 24.1795\n",
      "Step 500 / 547, loss = 18.4435\n",
      "Train loss: 22.210445\n",
      "Validation loss: 139.629959 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 53\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 23.1923\n",
      "Step 200 / 547, loss = 23.3244\n",
      "Step 300 / 547, loss = 19.2939\n",
      "Step 400 / 547, loss = 21.7566\n",
      "Step 500 / 547, loss = 25.0165\n",
      "Train loss: 20.694771\n",
      "Validation loss: 48.718460 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 54\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.4172\n",
      "Step 200 / 547, loss = 19.4574\n",
      "Step 300 / 547, loss = 23.3119\n",
      "Step 400 / 547, loss = 20.5592\n",
      "Step 500 / 547, loss = 16.7239\n",
      "Train loss: 19.545395\n",
      "Validation loss: 33.240871 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 55\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.3963\n",
      "Step 200 / 547, loss = 16.0200\n",
      "Step 300 / 547, loss = 15.5391\n",
      "Step 400 / 547, loss = 18.7014\n",
      "Step 500 / 547, loss = 16.3560\n",
      "Train loss: 18.645832\n",
      "Validation loss: 41.224831 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 56\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 23.1389\n",
      "Step 200 / 547, loss = 25.2599\n",
      "Step 300 / 547, loss = 22.1972\n",
      "Step 400 / 547, loss = 19.2278\n",
      "Step 500 / 547, loss = 27.7818\n",
      "Train loss: 21.634289\n",
      "Validation loss: 34.395184 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 57\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 21.3495\n",
      "Step 200 / 547, loss = 23.3404\n",
      "Step 300 / 547, loss = 18.1380\n",
      "Step 400 / 547, loss = 17.0137\n",
      "Step 500 / 547, loss = 19.7471\n",
      "Train loss: 21.207495\n",
      "Validation loss: 31.352604 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 58\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.9111\n",
      "Step 200 / 547, loss = 21.0646\n",
      "Step 300 / 547, loss = 16.3779\n",
      "Step 400 / 547, loss = 17.2827\n",
      "Step 500 / 547, loss = 16.9134\n",
      "Train loss: 19.950064\n",
      "Validation loss: 28.729252 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 59\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 19.7009\n",
      "Step 200 / 547, loss = 19.7725\n",
      "Step 300 / 547, loss = 16.8398\n",
      "Step 400 / 547, loss = 18.3790\n",
      "Step 500 / 547, loss = 13.3955\n",
      "Train loss: 18.630713\n",
      "Validation loss: 30.171574 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 60\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.0694\n",
      "Step 200 / 547, loss = 22.5240\n",
      "Step 300 / 547, loss = 16.2492\n",
      "Step 400 / 547, loss = 15.6720\n",
      "Step 500 / 547, loss = 18.9935\n",
      "Train loss: 17.590519\n",
      "Validation loss: 29.718143 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 61\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 26.8743\n",
      "Step 200 / 547, loss = 21.4266\n",
      "Step 300 / 547, loss = 22.7129\n",
      "Step 400 / 547, loss = 20.4928\n",
      "Step 500 / 547, loss = 16.9307\n",
      "Train loss: 20.744204\n",
      "Validation loss: 36.112907 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 62\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.1636\n",
      "Step 200 / 547, loss = 21.7248\n",
      "Step 300 / 547, loss = 18.9633\n",
      "Step 400 / 547, loss = 23.6909\n",
      "Step 500 / 547, loss = 30.2729\n",
      "Train loss: 20.313614\n",
      "Validation loss: 29.776978 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 63\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.5378\n",
      "Step 200 / 547, loss = 17.5232\n",
      "Step 300 / 547, loss = 17.0421\n",
      "Step 400 / 547, loss = 17.2690\n",
      "Step 500 / 547, loss = 15.7498\n",
      "Train loss: 19.050318\n",
      "Validation loss: 37.006622 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 64\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.4535\n",
      "Step 200 / 547, loss = 16.2887\n",
      "Step 300 / 547, loss = 16.1844\n",
      "Step 400 / 547, loss = 17.9626\n",
      "Step 500 / 547, loss = 15.9626\n",
      "Train loss: 17.787140\n",
      "Validation loss: 39.569721 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 65\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.3240\n",
      "Step 200 / 547, loss = 18.0741\n",
      "Step 300 / 547, loss = 15.8570\n",
      "Step 400 / 547, loss = 15.1622\n",
      "Step 500 / 547, loss = 18.3442\n",
      "Train loss: 17.193275\n",
      "Validation loss: 67.940498 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 66\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.9589\n",
      "Step 200 / 547, loss = 21.3318\n",
      "Step 300 / 547, loss = 18.7513\n",
      "Step 400 / 547, loss = 26.4200\n",
      "Step 500 / 547, loss = 22.1674\n",
      "Train loss: 19.754347\n",
      "Validation loss: 31.787678 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 67\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.4584\n",
      "Step 200 / 547, loss = 28.3033\n",
      "Step 300 / 547, loss = 17.6068\n",
      "Step 400 / 547, loss = 20.0153\n",
      "Step 500 / 547, loss = 20.8377\n",
      "Train loss: 19.268652\n",
      "Validation loss: 32.163860 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 68\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.4905\n",
      "Step 200 / 547, loss = 18.4820\n",
      "Step 300 / 547, loss = 15.3321\n",
      "Step 400 / 547, loss = 19.6766\n",
      "Step 500 / 547, loss = 17.2244\n",
      "Train loss: 18.429169\n",
      "Validation loss: 30.451145 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 69\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 21.4977\n",
      "Step 200 / 547, loss = 15.1413\n",
      "Step 300 / 547, loss = 14.9956\n",
      "Step 400 / 547, loss = 15.2505\n",
      "Step 500 / 547, loss = 19.2775\n",
      "Train loss: 16.984791\n",
      "Validation loss: 31.763119 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 70\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 15.9157\n",
      "Step 200 / 547, loss = 19.8826\n",
      "Step 300 / 547, loss = 17.9308\n",
      "Step 400 / 547, loss = 15.6198\n",
      "Step 500 / 547, loss = 15.5559\n",
      "Train loss: 16.257263\n",
      "Validation loss: 34.188290 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 71\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 18.4281\n",
      "Step 200 / 547, loss = 16.7147\n",
      "Step 300 / 547, loss = 18.9226\n",
      "Step 400 / 547, loss = 18.1254\n",
      "Step 500 / 547, loss = 15.1619\n",
      "Train loss: 19.310999\n",
      "Validation loss: 35.862530 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 72\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 19.6862\n",
      "Step 200 / 547, loss = 18.0023\n",
      "Step 300 / 547, loss = 17.6964\n",
      "Step 400 / 547, loss = 16.9746\n",
      "Step 500 / 547, loss = 22.6443\n",
      "Train loss: 18.675394\n",
      "Validation loss: 38.854130 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 73\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 14.9933\n",
      "Step 200 / 547, loss = 20.2774\n",
      "Step 300 / 547, loss = 15.5900\n",
      "Step 400 / 547, loss = 13.4940\n",
      "Step 500 / 547, loss = 18.0253\n",
      "Train loss: 17.610018\n",
      "Validation loss: 47.875492 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 74\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.0324\n",
      "Step 200 / 547, loss = 13.6168\n",
      "Step 300 / 547, loss = 14.8479\n",
      "Step 400 / 547, loss = 15.4909\n",
      "Step 500 / 547, loss = 16.8185\n",
      "Train loss: 16.089703\n",
      "Validation loss: 30.020645 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 75\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 12.9675\n",
      "Step 200 / 547, loss = 14.9127\n",
      "Step 300 / 547, loss = 14.6362\n",
      "Step 400 / 547, loss = 13.1533\n",
      "Step 500 / 547, loss = 15.0999\n",
      "Train loss: 15.453957\n",
      "Validation loss: 29.655603 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 76\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 15.0186\n",
      "Step 200 / 547, loss = 18.5711\n",
      "Step 300 / 547, loss = 20.2405\n",
      "Step 400 / 547, loss = 15.2096\n",
      "Step 500 / 547, loss = 18.7591\n",
      "Train loss: 18.332561\n",
      "Validation loss: 48.891685 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 77\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.2197\n",
      "Step 200 / 547, loss = 16.3557\n",
      "Step 300 / 547, loss = 15.2635\n",
      "Step 400 / 547, loss = 17.9145\n",
      "Step 500 / 547, loss = 23.0580\n",
      "Train loss: 18.188204\n",
      "Validation loss: 54.379757 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 78\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.9604\n",
      "Step 200 / 547, loss = 18.1124\n",
      "Step 300 / 547, loss = 15.2361\n",
      "Step 400 / 547, loss = 16.0070\n",
      "Step 500 / 547, loss = 16.7911\n",
      "Train loss: 16.760523\n",
      "Validation loss: 36.009090 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 79\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.9185\n",
      "Step 200 / 547, loss = 16.2522\n",
      "Step 300 / 547, loss = 17.4111\n",
      "Step 400 / 547, loss = 15.8770\n",
      "Step 500 / 547, loss = 12.0030\n",
      "Train loss: 15.520123\n",
      "Validation loss: 33.056358 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 80\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 12.4716\n",
      "Step 200 / 547, loss = 13.3469\n",
      "Step 300 / 547, loss = 13.9683\n",
      "Step 400 / 547, loss = 13.3439\n",
      "Step 500 / 547, loss = 12.9895\n",
      "Train loss: 14.742634\n",
      "Validation loss: 32.245468 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 81\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 15.8334\n",
      "Step 200 / 547, loss = 16.9096\n",
      "Step 300 / 547, loss = 15.0389\n",
      "Step 400 / 547, loss = 23.8732\n",
      "Step 500 / 547, loss = 14.6002\n",
      "Train loss: 17.648613\n",
      "Validation loss: 74.603790 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 82\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.5308\n",
      "Step 200 / 547, loss = 14.6249\n",
      "Step 300 / 547, loss = 17.1999\n",
      "Step 400 / 547, loss = 17.9723\n",
      "Step 500 / 547, loss = 15.3043\n",
      "Train loss: 17.807766\n",
      "Validation loss: 197.689575 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 83\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 23.5251\n",
      "Step 200 / 547, loss = 15.9703\n",
      "Step 300 / 547, loss = 14.2088\n",
      "Step 400 / 547, loss = 20.1606\n",
      "Step 500 / 547, loss = 10.9042\n",
      "Train loss: 16.369537\n",
      "Validation loss: 31.768549 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 84\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 13.6984\n",
      "Step 200 / 547, loss = 18.7411\n",
      "Step 300 / 547, loss = 14.9967\n",
      "Step 400 / 547, loss = 15.0099\n",
      "Step 500 / 547, loss = 12.3168\n",
      "Train loss: 14.960798\n",
      "Validation loss: 44.962769 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 85\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 12.8995\n",
      "Step 200 / 547, loss = 14.1873\n",
      "Step 300 / 547, loss = 15.1344\n",
      "Step 400 / 547, loss = 12.6730\n",
      "Step 500 / 547, loss = 18.0061\n",
      "Train loss: 14.272830\n",
      "Validation loss: 33.671467 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 86\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 17.2841\n",
      "Step 200 / 547, loss = 20.3668\n",
      "Step 300 / 547, loss = 19.7358\n",
      "Step 400 / 547, loss = 15.7494\n",
      "Step 500 / 547, loss = 20.9162\n",
      "Train loss: 17.216537\n",
      "Validation loss: 35.856499 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 87\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 15.0032\n",
      "Step 200 / 547, loss = 18.6687\n",
      "Step 300 / 547, loss = 15.1544\n",
      "Step 400 / 547, loss = 16.9552\n",
      "Step 500 / 547, loss = 17.7493\n",
      "Train loss: 17.056217\n",
      "Validation loss: 46.021809 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 88\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 20.7815\n",
      "Step 200 / 547, loss = 13.7489\n",
      "Step 300 / 547, loss = 11.4160\n",
      "Step 400 / 547, loss = 16.6842\n",
      "Step 500 / 547, loss = 12.9708\n",
      "Train loss: 15.975240\n",
      "Validation loss: 30.583754 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 89\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 13.4501\n",
      "Step 200 / 547, loss = 12.1627\n",
      "Step 300 / 547, loss = 11.7636\n",
      "Step 400 / 547, loss = 27.7185\n",
      "Step 500 / 547, loss = 16.0687\n",
      "Train loss: 14.377043\n",
      "Validation loss: 41.434864 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 90\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 11.0974\n",
      "Step 200 / 547, loss = 18.8607\n",
      "Step 300 / 547, loss = 11.1916\n",
      "Step 400 / 547, loss = 18.7746\n",
      "Step 500 / 547, loss = 19.0633\n",
      "Train loss: 13.659738\n",
      "Validation loss: 33.405067 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 91\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 14.5721\n",
      "Step 200 / 547, loss = 16.7725\n",
      "Step 300 / 547, loss = 19.9997\n",
      "Step 400 / 547, loss = 22.8804\n",
      "Step 500 / 547, loss = 15.8514\n",
      "Train loss: 16.843407\n",
      "Validation loss: 36.966019 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 92\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.2417\n",
      "Step 200 / 547, loss = 14.1264\n",
      "Step 300 / 547, loss = 18.9593\n",
      "Step 400 / 547, loss = 15.9402\n",
      "Step 500 / 547, loss = 21.0296\n",
      "Train loss: 16.534086\n",
      "Validation loss: 31.788517 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 93\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 14.4137\n",
      "Step 200 / 547, loss = 14.8806\n",
      "Step 300 / 547, loss = 13.0019\n",
      "Step 400 / 547, loss = 11.2616\n",
      "Step 500 / 547, loss = 14.0505\n",
      "Train loss: 15.452509\n",
      "Validation loss: 39.233105 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 94\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 11.7942\n",
      "Step 200 / 547, loss = 12.2843\n",
      "Step 300 / 547, loss = 15.1504\n",
      "Step 400 / 547, loss = 14.1491\n",
      "Step 500 / 547, loss = 14.5768\n",
      "Train loss: 14.059515\n",
      "Validation loss: 29.591448 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 95\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 12.3853\n",
      "Step 200 / 547, loss = 12.7443\n",
      "Step 300 / 547, loss = 9.5447\n",
      "Step 400 / 547, loss = 19.0082\n",
      "Step 500 / 547, loss = 12.4724\n",
      "Train loss: 13.309627\n",
      "Validation loss: 31.522633 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 6.597129976748661e-10\n",
      "Epoch 96\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 14.1104\n",
      "Step 200 / 547, loss = 19.6341\n",
      "Step 300 / 547, loss = 16.3172\n",
      "Step 400 / 547, loss = 13.4267\n",
      "Step 500 / 547, loss = 17.2478\n",
      "Train loss: 16.073456\n",
      "Validation loss: 42.565018 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0018096916275387072\n",
      "Epoch 97\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 16.8167\n",
      "Step 200 / 547, loss = 21.9121\n",
      "Step 300 / 547, loss = 15.3198\n",
      "Step 400 / 547, loss = 14.7843\n",
      "Step 500 / 547, loss = 11.6185\n",
      "Train loss: 16.156023\n",
      "Validation loss: 69.349823 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0013101092334768464\n",
      "Epoch 98\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 12.5447\n",
      "Step 200 / 547, loss = 13.6651\n",
      "Step 300 / 547, loss = 15.5239\n",
      "Step 400 / 547, loss = 13.3819\n",
      "Step 500 / 547, loss = 12.5467\n",
      "Train loss: 15.089771\n",
      "Validation loss: 40.019363 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.0006920756524520069\n",
      "Epoch 99\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 13.5664\n",
      "Step 200 / 547, loss = 15.8152\n",
      "Step 300 / 547, loss = 12.6835\n",
      "Step 400 / 547, loss = 14.4357\n",
      "Step 500 / 547, loss = 10.4496\n",
      "Train loss: 13.484338\n",
      "Validation loss: 30.971926 \n",
      "\n",
      "Saving checkpoint...\n",
      "lr: 0.00019165870622686566\n",
      "Epoch 100\n",
      "--------------------------\n",
      "Step 100 / 547, loss = 13.8021\n",
      "Step 200 / 547, loss = 14.9460\n",
      "Step 300 / 547, loss = 9.5492\n",
      "Step 400 / 547, loss = 14.8105\n",
      "Step 500 / 547, loss = 11.9211\n",
      "Train loss: 12.879183\n",
      "Validation loss: 35.664825 \n",
      "\n",
      "Saving checkpoint...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # if epoch % 3 == 0:\n",
    "    #     if low_lr:\n",
    "    #         actual_lr *= 0.8\n",
    "    #         optimizer.param_groups[0]['lr'] = actual_lr\n",
    "    #     else:\n",
    "    #         optimizer.param_groups[0]['lr'] = lr_min\n",
    "    #     low_lr = not low_lr\n",
    "    \n",
    "    print(f\"lr: {optimizer.param_groups[0]['lr']}\")\n",
    "    print(f'Epoch {epoch+1}\\n--------------------------')\n",
    "    loss = train(train_dataloader, model, criterion, optimizer, epoch)\n",
    "    train_epoch_loss[epoch] = loss\n",
    "    loss = test(valid_dataloader, model, criterion)\n",
    "    val_epoch_loss[epoch] = loss\n",
    "    # scheduler.step(loss)\n",
    "    # scheduler.step()\n",
    "    checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint, f'{epoch}.pth.tar')\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-6]\n",
    "while learning_rates[-1] < 1:\n",
    "    learning_rates.append(learning_rates[-1] * 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_lr_loss = np.zeros(len(learning_rates))\n",
    "n_epochs = 3\n",
    "for (i, lr) in enumerate(learning_rates):\n",
    "    print(f\"lr = {lr}\")\n",
    "    model = NeuralNetwork().to(device)\n",
    "    criterion = mean_absolute_percentage_error\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    # optimizer.param_groups[0]['lr'] = lr\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch+1}\\n--------------------------')\n",
    "        loss = train(train_dataloader, model, criterion, optimizer)\n",
    "        train_epoch_loss[epoch] = loss\n",
    "        epoch_lr_loss[i] = loss\n",
    "        # loss = test(valid_dataloader, model, criterion)\n",
    "        # val_epoch_loss[epoch] = loss\n",
    "        # checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "        # save_checkpoint(checkpoint, f'{epoch}.pth.tar')\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(learning_rates) + 1), epoch_lr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_boundary = np.array(epoch_lr_loss).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learning_rates[lr_boundary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max = learning_rates[lr_boundary] / 3\n",
    "lr_min = lr_max / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = val_epoch_loss.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 23\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(imputed_test)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, shuffle=False)\n",
    "best_model = val_epoch_loss.argmin()\n",
    "print(\"Best epoch:\", best_model)\n",
    "load_checkpoint(torch.load(f'{best_model}.pth.tar'))\n",
    "\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for x in test_dataloader:\n",
    "        x = x.to(device)\n",
    "        pred = model(x)\n",
    "        y_pred.append(pred.item())\n",
    "\n",
    "y_pred = pd.DataFrame(data={\n",
    "    'id': indexes, \n",
    "    'Predicted': y_pred}, index=indexes)\n",
    "\n",
    "pd.DataFrame(y_pred).to_csv('submission.csv',\n",
    "        columns=['id', 'Predicted'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x183221bd160>,\n",
       " <matplotlib.lines.Line2D at 0x183221bd190>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyHElEQVR4nO3deXxU9dU/8M/JZIOQQCABEsKmIoiKGigFayOKthRtcGuLWsXHtXWrPk+rWH9WbMWlqI9arRZ8FNxwV6wbAWrFqiDIJrIIypZMSELIQgJZJnN+f5y5yWQyk8x+586c9+s1rztz5947J3cmZ77zvd+FmBlKKaXiS5LZASillAo/Te5KKRWHNLkrpVQc0uSulFJxSJO7UkrFoWSzAwCAnJwcHjFihNlhKKWUpXz11VcHmDnX23MxkdxHjBiBtWvXmh2GUkpZChHt8fWcVssopVQc0uSulFJxSJO7UkrFIU3uSikVhzS5K6VUHNLkrpRScUiTu1JKxaEekzsRPUtElUS02W3dPCLaRkSbiOhtIurn9twdRLSTiLYT0U8jFLdSSgElJcCOHWZHEZP8KbkvBDDNY90yACcw8zgA3wK4AwCIaCyAmQCOd+3zdyKyhS1apZRyd/HFwNy5ZkcRk3pM7sy8EsBBj3UlzOxwPVwFoMB1fwaAV5i5mZl3AdgJYGIY41VKKXH4MHDwIFBWZnYkMSkcde5XAvjQdX8IgH1uz5W61nVBRNcS0VoiWltVVRWGMJRSCaW8vPNSdRJScieiOwE4ALxkrPKymdd5/Jh5PjNPYOYJublex71RSinfjBK73W5uHDEq6IHDiGgWgHMBTOWOiVhLAQx126wAgJ55pVT4GUm9pgZoagLS082NJ8YEVXInomkAbgdQzMyH3Z56F8BMIkojopEARgH4MvQwlVLKg3uJXatmuvCnKeRiAF8AGE1EpUR0FYAnAGQCWEZEG4joaQBg5m8AvAZgC4CPANzAzG0Ri14plbg0uXerx2oZZr7Yy+r/62b7uQC0bZJSKrLck7vWu3ehPVSVUtZktwOjR8t9Lbl3ocldKWVNdjswbhyQnKwldy80uSulrIdZEnpBATB4sJbcvdDkrpSynkOHgMZGID8fyMvTkrsXmtyVUtZjJPP8fLlpyb0LTe5KKesxeqcaJXdN7l1ocldKWY9nyb26GmhuNjemGKPJXSllPUZyz8uTGwDs329ePDFIk7tSynrsdiAzU275+R3rVDtN7kop67HbgSGu0cSNkrvWu3eiyV0pZT12e0eJXUvuXmlyV0pZj3tyz80FbDYtuXvQ5K6Ushajd6qR3JOSpJeqltw70eSulLKWgweBlpaO5A5oW3cvNLkrpazFvY27IT9fS+4eNLkrpazFvXeqQUvuXWhyV0pZi6+S+4EDUl2jAGhyV0pZjXvvVIP2Uu1Ck7tSylrsdqB/fyA9vWOdtnXvQpO7Uspa3JtBGrSXahea3JVS1uI+9IBBS+5daHJXSlmLt5J7bq50ZtKSeztN7kop62hrk4umnsndZtNeqh40uSulrKOqShK8Z3IHtK27B03uSinr8NbG3aC9VDvR5K6Usg5vvVMNWnLvRJO7Uso6eiq5V1VpL1UXTe5KKeuw2wEiYNCgrs8Zbd0rKqIbU4zS5K6Usg67HRg4EEhJ6fqckdy13h2AJnellJV4a+NuMNZrvTsATe5KKSvx1jvVoEMQdKLJXSllHd2V3AcOlF6qWi0DQJO7UsoqWluBykrfyT05WRK8ltwB+JHciehZIqokos1u6/oT0TIi2uFaZrs9dwcR7SSi7UT000gFrpRKMMZY7b6Su/GcltwB+FdyXwhgmse62QBWMPMoACtcj0FEYwHMBHC8a5+/E5EtbNEqpRJXd23cDdqRqV2PyZ2ZVwI46LF6BoBFrvuLAJzntv4VZm5m5l0AdgKYGJ5QlVIJrbveqQYtubcLts59EDOXA4BrOdC1fgiAfW7blbrWdUFE1xLRWiJaW1VVFWQYSqmE4W/JvapK6ucTXLgvqJKXdextQ2aez8wTmHlCbm5umMNQSsUdu12G9u0uX+TnA8zaSxXBJ/cKIsoDANey0rW+FMBQt+0KAOhvJKVU6Ox2KZkndZO2tK17u2CT+7sAZrnuzwKwxG39TCJKI6KRAEYB+DK0EJVSCt23cTfodHvtknvagIgWA5gCIIeISgHcDeABAK8R0VUA9gL4BQAw8zdE9BqALQAcAG5g5rYIxa6USiR2O3Dssd1voyX3dj0md2a+2MdTU31sPxfA3FCCUkqpLux2YMqU7rcZNEhGjdSSu/ZQVUpZwJEjQE1Nz9Uy2ku1nSZ3pVTsM5J1T8nd2EZL7prclVIW4E8bd4P2UgWgyV0pZQX+9E41aMkdgCZ3pZQVBFpyr6wEHI7IxhTjNLkrpWKf3Q6kpQHZ2T1vq71UAWhyV0pZgdGBibyNcOJB27oD0OSulLICf3qnGrSXKgBN7kopK+hu7lRPWnIHoMldKWUFgZTctZcqAE3uSqlYd+gQ0NDgf3JPSZFhgbXkrpRSMSyQZpAGbeuuyV0pFeOCSe7aS1WTu1IqxgXSO9WQn6/J3ewAlFKqW8GW3CsqgLbEnU5Ck7tSKrbZ7UCfPkBmpv/75OcDTqcMQ5CgNLkrpWJbIM0gDUZb9wS+qKrJXSkV20JJ7glc767JXSkV24JJ7joEgSZ3pVQMYw5s6AHDoEGy1JK7UkrFoJoaoLk58JJ7aqr0UtWSu1JKxaBgmkEaErwjkyZ3pVTsCiW5J/gQBJrclVKxK5jeqQYtuSulVIwySt5G08ZA5OcD+/cnbC9VTe5Kqdhlt8u8qb16Bb5vXp70Uq2qCn9cFqDJXSkVu4Jp425I8LbumtyVUrErlOSe4L1UNbkrpWKXltyDpsldKRWbnE4pdQfaO9UweLAsteSulFIxpKpKWroEW3JPTQVycrTkrpRSMSWUDkyGBG7rrsldKRWbwpHcE7iXqiZ3pVRsCqV3qkFL7sEholuJ6Bsi2kxEi4konYj6E9EyItrhWmaHK1ilVAIxStzGhdFgJHAv1aCTOxENAXAzgAnMfAIAG4CZAGYDWMHMowCscD2OiOrD1Xh+4/OobEzceRKVilt2OzBwIJCSEvwx8vIksR84EL64LCLUaplkAL2IKBlAbwB2ADMALHI9vwjAeSG+hk/f13yPWe/Mwr93/ztSL6GUMksobdwNCdzWPejkzsxlAB4CsBdAOYA6Zi4BMIiZy13blAMY6G1/IrqWiNYS0dqqIMd+OGnwSUhPTseq0lVB7a+UimHhSO4J3Es1lGqZbEgpfSSAfAAZRPRrf/dn5vnMPIGZJ+Tm5gYVQ6otFRPyJ+CL0i+C2l8pFcO05B6SUKplzgKwi5mrmLkVwFsATgVQQUR5AOBaRrRCfNKQSVhXvg7NjuZIvoxSKppaW4HKyuB7pxoSuJdqKMl9L4BJRNSbiAjAVABbAbwLYJZrm1kAloQWYvcmD52MlrYWrN+/PpIvo5SKpooKmRw71JJ7WhowYIAm90Aw82oAbwBYB+Br17HmA3gAwNlEtAPA2a7HETO5YDIA4It9WjWjVNwIRwcmQ16eVssEipnvZuYxzHwCM1/GzM3MXM3MU5l5lGt5MFzBepOXmYfhfYdrvbtS8SScyT0/X0vuVjWpYJK2mFEqnoSjd6pBS+7WNblgMvbV70NZfZnZoSilwsFuB2w2IMiWdJ0YvVSdztCPZSHxkdyHuurdtWpGqfhgt0tLF5st9GPl5QEOR8L1Uo2L5H7y4JORZkvTqhml4kU42rgbErQjU1wk91RbKsbnj9eSu1LxIpzJPUE7MsVFcgek3v0r+1doaWsxOxSlVKi05B6yuEnukwomobmtGRv2bzA7FKVUKJqagIMHQ++dajCSu5bcLWTPHuCPfwR279bOTErFC6OEHa6Se3o6kJ2tJXdLaWgA7r8f+PRTDMkagqFZQ7XeXSmrC2cHJkMCTrdn7eQ+ZgzQpw/w5ZcAtDOTUnEhEsk9Aafbs3Zyt9mACROANWsAyEXVPXV7UH4osd5EpeJKOHunGrTkbkETJwLr1wMtLdqZSal4YLcDqalA//7hO2ZeXsL1Uo2P5N7SAmzahFMGn4JUW6peVFXKyoxmkEThO2Z+vowRX10dvmPGOOsn9x/8QJZffom05DQU5hViVZnWuytlWeFs425IwLbu1k/uQ4cCgwa1X1SdXDAZa+1rtTOTUlYVieSegL1UrZ/ciaRqxq3FTJOjCRv3bzQ5MKVUULTkHhbWT+6AJPdt24C6uvbOTNokUikLOnRIbpFK7lpyt5iJE2W+xa++wtC+QzEkc4i2mFHKioySdbiGHjD06gX066cld8uZMEGWblUzmtyVsqBIdGAyJFhb9/hI7v37A6NGdbqourt2N/Y37Dc5MKVUQCKZ3BOsl2p8JHeg00VVozOT1rsrZTGR6J1q0JK7RU2cKB8Mux2FeYVISUrRzkxKWY3dDmRkAJmZ4T+20UuVOfzHjkHxk9yNzkxr1iA9OR2n5J2inZmUsppI9E415OdLb/aDB8N/7BgUP8n95JOB5ORO9e5rytagta3V3LiUUv6LRBt3Q4I1h4yf5N6rFzBuXKcWM0ccR7CpYpPJgSml/BbJ5G4cN0EuqsZPcgek3n3NGsDp1M5MSlkNs5bcwyj+kntdHbBjB4b1HYa8Pnna3l0pq6itlflTI53cteRuQRMnyvLLL0FE2plJKSsxStTh7p1q6N0b6NtXS+6W5DHt3uSCyfi+5ntUNlaaHJhSqkeR7MBkyM/XkrslGdPuaWcmpawnGsk9L09L7pY1cSKwYQPQ3IzxeeORnJSsnZmUsgKjd6pRNx4JWnK3MLdp93ql9MLJg0/WenelrMBul5Ebe/eO3GsYJfcE6KUaf8ndracq4OrMZF8Dh9NhYlAR4nAADz4IVOo1BRUHItkM0mD0Uq2piezrxICQkjsR9SOiN4hoGxFtJaLJRNSfiJYR0Q7XMjtcwfrFy7R7h1sP4+uKr6MaRlS89RYwezbw7LNmR6JU6KKR3BOorXuoJffHAHzEzGMAnARgK4DZAFYw8ygAK1yPo8fLtHsA4rNq5oknZLlypblxKBUO0UzuCVDvHnRyJ6IsAEUA/g8AmLmFmWsBzACwyLXZIgDnhRZiENym3RvRbwQGZQyKvxYzmzYBn34q7Xb/8x+polHKqpxOSbjRqJYBtOTeg6MAVAF4jojWE9EzRJQBYBAzlwOAaznQ285EdC0RrSWitVVVVSGE4YXbtHtEhMlDJ8dfyf3JJ4H0dOC++2TOyY0WnBC8rg4YPBh47TWzI1FmO3BACihacg+bUJJ7MoBCAE8x8ykAGhFAFQwzz2fmCcw8ITc3N4QwvPCcdm/IJOw8uBNVjWH+EjFLbS3w4ovAJZcAM2bIOitWzSxfDlRUAM8/b3YkymyR7p1qyMgAsrK05N6DUgClzLza9fgNSLKvIKI8AHAto9+Uw3PaPVdnptVlq7vbyzoWLgQOHwZuuEH+GY4+2prJvaRElsuXA42N5saizBWNDkyGBJluL+jkzsz7AewjotGuVVMBbAHwLoBZrnWzACwJKcJguV1UnZA/ATayxUdnJqdTqmROPRUoLJR1RUVS/+50mhtbIJiBpUvln7m5GVi2zOyIlJmimdwTZLq9UFvL3ATgJSLaBOBkAPcBeADA2US0A8DZrsfRZ0y7V1aG3im9cdLgk+Kj3n3ZMmDnTim1G4qKgOpqYOtW8+IK1I4dwJ49wO23S8eVJeaUAVSMMHqnDh4c+ddKkJJ7cig7M/MGABO8PDU1lOOGhXtnpiFDMLlgMhZuWIg2ZxtsSTZzYwvFE09IO/6LLupYd/rpsly5Ejj+eHPiCpRRJXPOOcDq1cB77wFtbTI+kEo8djuQmwukpkb+tYySO3NkpvOLEfHXQ9VgTLvn1lO1sbURmys3mxtXKHbtAt5/H7jmms7/BCNGAAUF1qp3X7pUrhUcfTRQXCytJVbFWXNV5b9otHE35OVJVWBtbXRezyTxm9y9TLsHWLwz01NPAUlJwHXXdV5PJFUzn3xijTEzWlqAjz8GfvITeTxtGpCSArz7rrlxKfNEM7knSFv3+E3uQKdp947KPgq5vXOt25np8GHgmWeA88+XUrqnoiKpR/zuu+jHFqgvvpDWMUZy79sXmDJFk3sii3bJHYj7evf4T+6uafcs35nplVdksCP3C6nuiopkaYWqmaVLpW79zDM71hUXS6/ib781Ly5lDodD+jtoyT2s4j+5A506M31b/S2qD1ebGFQQmOVC6vHHd1w89TRmDJCTY43kXlICTJ4snUkMP/+5LLX0nngqKuQzriX3sIrv5O457Z5VZ2ZatQpYvx648UbfV/eNevdYT+5VVcC6dcBPf9p5/fDhwEknaXJPRNHqnWro0wfIzNSSu6V5TLv3g/wfIImSrJfcn3hCSrm//nX32xUVSYuaffuiE1cwli+XUppR3+6uuBj47DNpOaMSRzQ7MBkSoK17fCd3oNO0exmpGRg3aJy16t0rKoDXXweuuEJKHN0xqmw+/TTiYQWtpESGhxg/vutzM2ZIL9sPPoh+XMo8ZiT3BJhuLzGSu2vaPUDau68uW402Z5vJgflpwQKgtRW4/vqetz3xRGl58sknkY8rGMyS3M86y3tnpcJC+afT3qqJpaxMmvgO9DqAbGQkwETZ8Z/cjZ6qbjMzNbQ0YEvVFhOD8pPDATz9NHD22cDo0T1vb7MBp50Wu/Xu33wj/1DeqmQAuW5QXCytaZqaohubMo/dLsMORLN3slFyt0K/kCDFf3L3mHbPUp2ZliyRUs2NN/q/T1GRNCmMxXlVjSEHfCV3QJJ7Y6N0clKJIZpt3A15ecCRI9JUOk7Ff3I3pt1zDUNwTP9jkNM7xxrJ/YknpBXJOef4v4/R3j0W691LSoDjjpMvXF/OOEPG3NZWM4nDjORuvF4c17vHf3IHOk27R0SYVDAp9lvMfPMN8O9/A7/9bWA/VwsLgd69Y69q5sgRuRbQXakdkNmlpk2T5B7HP5mVG7NK7sZrx6nESe6uafcA6cy07cA2HDxy0OTAuvHkk0BaGnDVVYHtl5oqHYRiLbn/5z9Sj95TcgekasZub3+/VBxrbpbhqrXkHnaJkdw9pt1rn5mpNEZnZqqrk6nnZs6UXqeBKiqSOVVjadS7khL54vHVw9bd9OnSeiLeq2bq64HbbkvsLzEjuWrJPewSI7l7TLsX852Znn9eLioGciHV3emnyy+Vzz4Lb1yhWLpUWvJkZPS8bU4O8KMfxXdy//ZbYNIkYN484OqrE7cKKtq9Uw2ZmdJvREvuccBt2r3MtEycMPCE2Lyo6nTKhdSJEzt+cQRq4kQpJcdKe/fycuDrr/2rkjEUF8uvjz17IheXWT78UN6jykr5At+wAXjnHbOjMocZHZgMcd7WPbGSu2vaPaCjM5OTY2ze0RUrpFQXbKkdkLHsJ06MnXp3Y35Uz/FkujNjhiz/+c/wx2MWZuCBB6T104gRwNq1wP/+L3DsscDdd1trDtxwMTO5x3kv1cRJ7u7T7kGSe31zPbZWxdi8o08+KdON/eIXoR2nqEjqchsawhNXKJYuld6H48b5v8+oUTLwW7xUzTQ2yjWUO+4AfvlLqTIbMUJmC7v7bvll8+abZkcZfWVlMlHLgAHRf20tuccJY9q9WO7MtGePlFSvvlqaBIaiqEh6uJo9dZ3TKSX3s8+Wi6SBKC6W5qBW72iyaxdw6qkyRtCDDwKLF3e+9vCrX0n7/zlzZB7ZRGI0gzRjLtM476WaOMndY9q9Ywcci/69+uOLfSEm9y1bZOyXcHj6aVn+5jehH+vUUyWZml01s2GDDPMbSJWMobhYzu1HH4U9rKhZsUKunezdKwOi3XZb10Rms0li37IFeO01U8I0jRlt3A15eTLDWX29Oa8fYYmT3IFO0+61d2YqC6Fku2iRTKAxapTMbxrKeChNTTJI2IwZwLBhwR/HkJkpHZrMTu7GkANnnRX4vpMmSRVVpKpmmIF//EN+LYXrC9r92I8+Kl9qgwfL527aNN/bX3QRcMIJkuQdjvDGEsvMTu5A3Na7J15yr68HduwAIJ2ZtlRtQW1TbeDH2rRJStiTJsmH5PrrgaOPln/ow4cDP96rr0pnDl/T6AWjqEiqZZqbw3fMQJWUyC8m4x8pEDYbcO65UuINd/IFgJdekvewuFjiu/FGOV+h/kw/cgSYNQu49VaZYWrVKuCYY7rfJykJuOceuZi+eHFor28VmzbJ/+Kxx5rz+vE+3R4zm34bP348R8XmzcwA8/PPMzPzsu+WMeaAX9j4QmDHqa1lPuYY5rw85v37mZ1O5hUrmKdMkePn5jLffz9zXZ3/x/zBD5jHjJFjhcs770g8n34avmMG4tAh5pQU5j/8IfhjvP22/A3/+lfYwmJm5spK5gEDmCdNYn73XeZf/Yo5PV1e6+ijme++m/nbbwM/7t69zOPHy3H+/Gfmtjb/921rYz75ZPlstbYG/tpW4nDIZz43l/nAAXNi2LpV3qcXXzTn9cMAwFr2kVdNT+wczeTucDD36cN8443MzNziaOHx/xjP/R/sz/vq9vl3DKeT+YILmG025pUruz7/6afM06bJqc3OZp4zh/ngwe6PuXq1bP+3vwX4B/WgulqOe++94T2uv957T15/2bLgj9HQwJyWxvy734UtLGZmvuQS+eLZvLljXV0d83PPMU+dykwksf/wh/K+VFb2fMyVK5kHDmTOzGResiS4uJYskdd97rng9reKRx6Rv3PxYvNiqKuTGP76V/NiCJEmd3dTpjBPnNj+8NsD33LG3Aw+/bnT2dHm6Hn/hx6S0/bQQ91vt2YN84wZsm1mJvPs2b4TxGWXyZdOICV9f514IvNPfhL+4/rj5puZe/ViPnIktOOccw7zyJHh+1Xz/vvyvsyZ43ub0lLmefOYTzpJtrXZJI7Fi5kbGztv63Qy//3vzMnJzKNGMW/ZEnxsTqeU/EeOZG5pCf44sez775l792Y+99zw/lINlNMpcdxyi3kxhEiTu7vbbmNOTWVuampf9dz65xhzwPd+0kMJd+VK+Se/4AL/P5QbN8pPfiJJdLfeylxW1vF8ZaXEc8MNQfwxfrjhBuaMDHN+5o8eLb9iQjV/vnxUv/469GPV1zMPHco8dmynz0C3Nm1ivv125oICiaNPH+ZZs+QXSWMj89VXy/rp05lrakKP0fjyWbAg9GPFGqeT+eyz5Rzu3Wt2NMyTJ0s1mMOPgl0M0uTu7o035M/+8sv2VU6nk2e+MZNt99j4872fe9+vvFzq2I85RurcA7V1K/Pll8uXQ1oa8/XXM+/ezXzffRLPN98E+Qf14NVXu/y9UbF7t7zuI4+Efiy7XY41d27ox7rpJvmi/dzH+9ydtjbmjz9mvuoq5qwsiSktTZZ33hm+BOF0SnXQsGHMzc3hOWasWLRIztcTT5gdiXjrLYnn5ZfNjiQomtzd7dnj9cNVe6SWRzw6gkc8OoJrj3gk79ZWqc7p1UtK4qH47jvma6+V+t7kZKmyOfPM0I7ZHSMx9lSNFG4LFoT3S2viREl4ofj8c0nsN90UejyHDzO//jrzFVdIggi3jz6S8/fUU+E/tlkqKpj792c+9dTALjRHUlsb8/HHyy1WYgqAJnd3TifzoEFSivbw+d7P2XaPjWe+MZOd7tUus2fLqVq4MHxx7N0rSaZfP+alS8N3XG9GjWIuLo7sa3i66CLmIUPCV6d6773yHtjtwe3f1MR83HFSGq6vD09MkeR0ShIsKAj9mkWsmDlTqiBDuSYRCS+/LJ+tN980O5KAaXL39POfS7NDL+795F7GHPBz65+TFe++K6fpmmuiF1+4XXWVtNyJVsnE4ZAvrf/6r/Adc9MmeR/mzw9u/7vvlv0/+CB8MUXa8uUckVZUZvjnP7m9eWiscTikAHTKKeZe4A2CJndPf/mL/Ole6s4dbQ6esnAKZ8zN4O/XLpckVVho7dKTUc+5aVN0Xm/VKnm9V14J3zGdTmlBcu65ge+7ebNUg116afjiiQank7moSK71HD5sdjTBq6uTXyAnnBC71xCee04+s++/b3YkAekuuSdWD1XDxImyXLu2y1O2JBteOP8FZHEqmi4oBgPAG2+EPpCXmYzZj6I1vvvSpTJ+ytSp4TsmkfQkXb5cRlj0V1ubDMSWlSXD61oJEfDnP0v3+H/8w+xognfHHTL64zPPyDwDsejSS2Uy+r/8JW4GEgs5uRORjYjWE9F7rsf9iWgZEe1wLbNDDzPMjEkwXMP/eirIKsDnGyfguL2H8dytZwAjR0YxuAgYPlzGq4nWODMlJcD48cFNEdid4mIZg2f5cv/3+fvfpfv/Y4/JODVWc/rpwJlnAvffH9iXWqz47DN5D26+GfjhD82OxreUFGD2bPmsfPyx2dGERThK7r8D4D4o+mwAK5h5FIAVrsexxWPavS4WLcKI15fho1+cgqv4bZR8VxLd+CKhqEiSe6RLJXV18g8SzCiQPfnxj4G+fYElS/zbfs8eKTX+7GfAJZeEP55ouecembXpqafMjiQwzc3yq2n4cODee82OpmdXXCHjzVghVj+ElNyJqADAOQCecVs9A8Ai1/1FAM4L5TUixm3avU6MAcHOOANFi/6Nsbljcfnbl6OysTL6MYZTURFQUdE+aFrE/OtfUhUSyJR6/kpJkcmz33uv53HPmYHf/lbuP/WUOeOFh8tpp8n5fPDB2Jh8xV9z5wLbtkmVUp8+ZkfTs/R04A9/kJJ7LM0/HKRQS+6PArgNgPv8YIOYuRwAXMuBIb5GZHhMuwdASp0XXghkZwOLF6N3ryy8cuErqG2qxRXvXBF7U/IFoqhIlpGumikpkX/kSZMic/wZM2R8+NWru9/u5ZdlrtL77pOSo9Xdcw9w4IDMr2sFX38tVUmXXRaZX3GRcs01Un0XB6X3oJM7EZ0LoJKZvwpy/2uJaC0Rra2qqgo2jOB5TLsHZuDKK2XWnNdeAwYNAgCcOOhEPPyTh/Hhzg/x+OrHox9nuBx7rEx1F43kfuaZkbtwNm2azKjV3RjvVVXA734nXzDhHELZTJMmya+WefNif3KJtjZJkv36AY88YnY0gcnIAP77v2WCGC8NLizFVzOanm4A7gdQCmA3gP0ADgN4EcB2AHmubfIAbO/pWFFvCsksTcuSk5nvuEMeGwOCPfxwl02dTicXLy7m1L+k8jr7uigHGkYXXcQ8fHjkjr9jB0ela/lZZ0mHJF8uvbTriI/xYM0aNnWUT389+qjE+dJLZkcSnLo66Rdy3nlmR9IjRLqdO4ApAN5z3Z8HYLbr/mwAf+1pf1OSO7O0X5861a8Bwaoaqzj/4Xwe/bfR3NDcEOVAw+Txx+Ut3707Msd/8kk5fjDjoAfC+Du8vc4HH8hzf/pTZGMwS3Gx9L0IZnyjaNi1S0ZanD7dch2COpkzJ7p9Q4LUXXKPRDv3BwCcTUQ7AJztehybjIuqv/qVNHd89lmfF95yeufghfNfwLfV3+KWj26JbpzhYrR3j1TVzNKlch57mnUoVMXFsvSsmjl0SC6GH3cc8Mc/RjYGs8yZA9TWyoxfsYZZzn9SkvUvYt90k0xVOXeu2ZEEz1fWj+bNtJL7s8/Kt3MAA4LNXjabMQf82ubXIhxcBLS1Sanv6qvDf+yWFhnG9brrwn9sb046SXpvurv5ZhkY7LPPohODWc4/X0al7GkSmGh74QWOm+ESmGVMKSLmbdvMjsQn6PADPhiTBrim3fNHi6OFJy6YyH3v78u7ayJUvRFJP/8587HHhv+4K1dyVAdfuusu5qSkjinajBEfXbNsxbWNG+Vc/7//Z3YkHYxpCydPtuzY6F1UVEjBb9YssyPxqbvknpjDDxhGjpSWB5dd5vcuKbYULL5wMZzsxKVvXQqH02Iz1RcVySTM+/eH97hLl8qE1meeGd7j+lJcDDidMnl2S4t0likokKaP8W7cOOAXv5Cqmepqs6MRt9wi/0vPPCOfg3gwcCBw3XXAiy9KKzqLSezkDgT1QTwq+yg8dc5T+GzfZ7h3pcXawxrt3T/9NLzHLSmR7uX9+oX3uL4UFkpvwiVLpD31li1Sz5uZGZ3XN9vdd8twBA89ZHYkwPvvS7+CO+8Exo41O5rw+v3vJUc8+KDZkQTOV5E+mjfTqmVCdNlbl3HSPUk86+1Z/MrXr/DBwzFWB+pNS4tMuxfO6osDB6RK5J57wndMf1x3nfxsTklhvvji6L52LLj4YnkvKyrMi8GYtvD442N3xMdQ/eY3Mg79vn1mR9IFtFomMp6c/iQuG3cZ3t3+Lma+ORM583Jw2rOn4b5P78P68vVyUSPWpKQAp54a3hYzy5dLS4lIDDnQnRkzgCNHZMTHxx6L7mvHgj/9Sf7+c84B1q+P/uu3tQH/8z9AaSmwYEHsjvgYqttvlyrAefPMjiQgmtxDkJmWiYXnLUTlHyrx2ZWf4Y+n/RFNjibc+a87UTi/EPmP5OPKJVfi9W9eR21Trdnhdigqku7hBw+G53glJVIdY4y2GS1nnCHjrsyfb80RH0M1ZgzwyivA3r1y7o1672j45BN5zQULgFtvBSZPjs7rmmHECLkuN3++jM9kFb6K9NG8WbVaxpfyQ+W8cP1C/uXrv+R+D/RjzAHb7rHxj5/9Md+38j7eUL6h8zR+0Wa0bFmyJPRjOZ0yEcNFF4V+LBWcgwel6oCIOT9f5naN1Odr1y55rwGZsvDVV63dWclf27dL66zbbjM7kk7QTbUMcQxUHUyYMIHXWn0cBx8cTgdWl67Ghzs/xAc7PsD6/fLzOT8zH9OOnoafjfoZJhdMRn5mPihanT6amqSkfeONoV+Q27IFOP54KdVcc01YwlNBWr1aOhFt2CDDHD/xBHDUUeE5dkMD8MAD8nmx2WTs89//HujVKzzHt4JLLgH++U9g925gwACzowEAENFXzOz1J7Mm9ygrP1SOpd8txQc7PkDJdyWoa64DAAzMGIjCvEKcMvgUFOYVojCvECP7jYxcwj/9dKmv9TWmvb8efVR+lu/eHR+jL1qdwyFJ/a675P5dd0kSDrY+3OkEXnpJkrndLjMWPfCANDtNNJs3AyeeKNc67rnH7GgAaHKPWQ6nA2vK1uCr8q+wrnwd1u9fj82Vm9vbzvdN64tT8k5B4eBCWeYVYvSA0bAlhaEd8V13SRPCmprQmg9Onw58/72M261iR2mp1MG/+aYMx/DUUx3DT/hr1SoZXfPLL2UU1ccei++6dX9ccIHMWbBnj0wcYzJN7hbS7GjG5srN7cl+Xfk6bKzYiCZHEwCgV3IvnDT4pE4J/7ic49ArJcCfx8uWSeuWpUsDb+XS2ipDJa9YIZ2GrrkGeNzCwyHHsw8+kGGPd+8GLr9cWnwM7GGKhbIyKam/+CKQlycl9V//WsaMSXRffSUXkufOjYnxizS5W5zD6cD2A9uxrnxde9Jfv3896pulZQSBMDJ7JMbmjsVxOcdhbO5YjM0dizE5Y5CVluX9oA0NUu8+e3bPExM4nfKTdMUKuX3yiexPJJ2JXn5ZxotXsenwYUlG8+bJRCoPPghcdVXXZH3kiNSpP/BARzPHO+6wxixK0TR9uvya2bNHxn83kSb3OORkJ76v+R7ry9djS9UWbD2wFVuqtmB79Xa0tLW0b1eQVdCe8N0T/4DeA6RHaVpa1zbvzFLVYiTzjz+WCTAAmXt26lS5nXFGzFxYUn7YskWmHly5UqpXnn5ahjJgBl5/HbjtNklYF14oXwRWnxg+Uj7/HPjRj4CHH5aJPUykyT2BOJwO7KrZ1Snhbz2wFVurtqKxtbF9u9zeuXj8X2m4cLkdjy/9M45OGoCxX+/HkC+3ofenq0B79siGeXkdyXzqVGDoUJP+suhrc7ahvrketU21qGuuQ21TbfutrqmufX1KUgoy0zLRJ7UP+qT2QWaq3Pe2LiM1A0lkYvUGM/D883KRtaZGqmzWr5fhKMaNk3r1KVPMi88qpk6VL8tdu2TuVZNocldwshP76vZ1JPyqrei34j+Y9/g27OoHjKyV7WrSgX+PJKwfm41d448CRo/GsL7DMazvsE63zDRrjeHicDpQ2ViJ/Q37u9xqmmo6JWzjdqjlUI/HzUjJgMPpQHNbs9+xZKRkdCT9tEwM6zsMYwaMweic0Rg9YDTG5IyRX1aRVF0tVS4LFgA5OVJtc9VV8TPoV6R9/LEMkvfkk8D115sWhiZ35V19PVBUBEdOf1RPPhk7TxmOb4akYk9DKfbW78XeOrmV1pd2Gf0yOz0bw/oOw9C+Q5HTOwfZ6dnol96vY9kru9P9fun90Cu5V1iadjIzWtpa0NzWjCOtR3Dg8IEuCbuisaLT4wOHD4DR9bOelZaFAb0GoF96P/RN74t+6f3kflrHfc/HxnZZaVlITkoGALS2taKhpQENLQ041HKo437zId/rWxtQ31yPXTW7sOPgjk7VaQN6DcDonNHtSX9MzhiMHjAaR2UfhRRbSsjnsN3evXLtJcvHtRnlHbP0jt63D9i507ShFzS5q5C0Oduwv2E/9tbtxZ66Pe1Jf2/dXuyr34fqw9WoaapBQ0tDt8dJtaV2Sfj90vvByU40O5rR3NaMJkdT+/1mh+ux677xvHsS9CbNlobBfQb3eBuUMSjwVkYR0uZsw+7a3dhevR3bDmzD9gPb2+9XNHZ0eU9OSsbR2Ud3KuUP7zscQ7KGID8z3/cFdBV+H34oF1fPPx8YPFguQjsccvN239fzZ50V9IxPmtxVVDicDtQ21aLmiFRz1DTVdLpvPNd+37W0kQ1pyWlIs6UhPTm9/X5asuuxreOx5zbpyenI6Z3TKWlnpWVFr7dvFNQ21bYn++0HtmNbtSR/z9I+APRJ7YMhmZLoh2QN6bifOaT98eA+g8Nb+k9UzDJo26pVUp2VnNyx9Oe+sZwyRVqtBUGTu1JxyCjt76vfh7L6MtgP2VF2qAxlh1z3Xetana2d9iMQBmYMbC/tF2QWtFexDc0aiqF9h2JI5hCkJaeZ9JeFrrGlEZWNle23qsNVPh+3trUiOSkZtiQbkpOSO91s5GWdx3bZ6dkoyCrA0KyhKMgqaL/179U/4oUMTe5KJSgnO1F9uFqSvvsXQH1Z+xdBaX0pDh7pOkLooIxBnZO+K/EPzRqKYX2HYXCfwX71lmZmtHEbWtta0epshcPp6HTfqHJraWvpVCXnuTSus3g+V9NUg6rGzsn7iOOI11j6pPbBwIyByO2d275MS06Dw+mAw+lAG7e133e/tTm7rjf+puoj1bAfssPJzk6vlZ6c3inZF2TKcmjfji+BnN45IbWe0uSulOpWY0sjSutLsa9+n1xLqduHffWum+u+5zWV5KRk5GfmI82W5jVpu9+PhJSkFKQlpyE7PRu5GZKsB2YMxMDeAzs/diXx3Ixc9E7pHZFYHE4HKhoqUFpf2n4ejfvGrexQWZdzkWpLxWXjLsMzxc8E9brdJffkoI6olIorGakZcpE2Z7TX55kZtU21nZK9sWxpa0GKLQUpSXJLTkrueGxzPXbd93w+OSkZaclpSLWldrqu0tMyxZZibn8BD8lJyXJNI2sIfogfet3GyU5UNlZ2SfqjB3g/56HSkrtSSllUdyX32PnqU0opFTaa3JVSKg5pcldKqTikyV0ppeKQJnellIpDmtyVUioOaXJXSqk4pMldKaXiUEx0YiKiKgB7zI6jGzkADpgdRDc0vtBofKHR+EITSnzDmTnX2xMxkdxjHRGt9dULLBZofKHR+EKj8YUmUvFptYxSSsUhTe5KKRWHNLn7Z77ZAfRA4wuNxhcajS80EYlP69yVUioOacldKaXikCZ3pZSKQ5rcARDRUCL6mIi2EtE3RPQ7L9tMIaI6Itrguv0pyjHuJqKvXa/dZWYTEo8T0U4i2kREhVGMbbTbedlARPVEdIvHNlE/f0T0LBFVEtFmt3X9iWgZEe1wLbN97DuNiLa7zmdwU9MHF988Itrmeg/fJqJ+Pvbt9vMQwfjmEFGZ2/s43ce+Zp2/V91i201EG3zsG9Hz5yunRPXzx8wJfwOQB6DQdT8TwLcAxnpsMwXAeybGuBtATjfPTwfwIQACMAnAapPitAHYD+lcYer5A1AEoBDAZrd1fwUw23V/NoAHffwN3wE4CkAqgI2en4cIxvcTAMmu+w96i8+fz0ME45sD4Pd+fAZMOX8ezz8M4E9mnD9fOSWanz8tuQNg5nJmXue6fwjAVgBDzI0qYDMAPM9iFYB+RJRnQhxTAXzHzKb3OGbmlQAOeqyeAWCR6/4iAOd52XUigJ3M/D0ztwB4xbVfxONj5hJmNmZRXgWgINyv6y8f588fpp0/AxERgF8CWBzu1/VHNzklap8/Te4eiGgEgFMArPby9GQi2khEHxLR8dGNDAyghIi+IqJrvTw/BMA+t8elMOcLaiZ8/0OZef4Mg5i5HJB/QAADvWwTK+fySsivMW96+jxE0o2uaqNnfVQrxML5+zGACmbe4eP5qJ0/j5wStc+fJnc3RNQHwJsAbmHmeo+n10GqGk4C8DcA70Q5vB8xcyGAnwG4gYiKPJ4nL/tEtZ0rEaUCKAbwupenzT5/gYiFc3knAAeAl3xs0tPnIVKeAnA0gJMBlEOqPjyZfv4AXIzuS+1ROX895BSfu3lZF/D50+TuQkQpkDfhJWZ+y/N5Zq5n5gbX/Q8ApBBRTrTiY2a7a1kJ4G3ITzd3pQCGuj0uAGCPTnTtfgZgHTNXeD5h9vlzU2FUV7mWlV62MfVcEtEsAOcCuJRdlbCe/Pg8RAQzVzBzGzM7ASzw8bpmn79kABcAeNXXNtE4fz5yStQ+f5rc0V4/938AtjLzIz62GezaDkQ0EXLuqqMUXwYRZRr3IRfdNnts9i6Ay0lMAlBn/PyLIp+lJTPPn4d3Acxy3Z8FYImXbdYAGEVEI12/Rma69os4IpoG4HYAxcx82Mc2/nweIhWf+3Wc8328rmnnz+UsANuYudTbk9E4f93klOh9/iJ1tdhKNwCnQX72bAKwwXWbDuA3AH7j2uZGAN9ArlyvAnBqFOM7yvW6G10x3Ola7x4fAXgScpX9awATonwOe0OSdV+3daaeP8gXTTmAVkhp6CoAAwCsALDDtezv2jYfwAdu+06HtHD4zjjfUYpvJ6S+1fgcPu0Zn6/PQ5Tie8H1+doESTh5sXT+XOsXGp87t22jev66ySlR+/zp8ANKKRWHtFpGKaXikCZ3pZSKQ5rclVIqDmlyV0qpOKTJXSml4pAmd6WUikOa3JVSKg79f079AZ26775bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, n_epo + 1), train_epoch_loss[:cnt], 'g', range(1, cnt + 1), val_epoch_loss[:cnt], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.946487"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_epoch_loss[best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestRegressor(n_estimators=1000)\n",
    "rf_clf.fit(imputed_train_X, train_y)\n",
    "y_pred = rf_clf.predict(imputed_val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_pred, torch.tensor(np.array(val_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
